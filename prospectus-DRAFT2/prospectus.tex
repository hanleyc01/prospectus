\documentclass{article}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{hyperref}
\usepackage[style=apa]{biblatex}
\usepackage[shortlabels]{enumitem}
\usepackage{graphicx}
\bibliography{sources.bib}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\newtheorem{theorem}{Theorem}[subsection]
\newtheorem{corollary}{Corollary}[subsection]
\newtheorem{lemma}{Lemma}[subsection]
\newtheorem{example}{Example}[subsection]
\theoremstyle{definition}
\newtheorem{definition}{Definition}[subsection]

\title{Investigating Hebbian Alternatives to Dense Associative Memory}
\author{Connor Hanley}
\date{\today}

\begin{document}
\maketitle

\begin{abstract}
  Dense Associative Memories generalize traditional Hopfield networks
  while providing a substantially increased capacity. To do this,
  they show that by increasing the separation of similarity scores
  between query patterns and stored patterns. The cost of increased
  capacity is simplicity and biological plausibility. We propose
  to investigate the capacity characteristics of all Hebbian
  associative memories, to see if there exists any alternative to
  Dense Associative Memories which maintains the simplicity and
  appeal of Hopfield networks.
\end{abstract}

Humans are able to recognize and retrieve patterns of data using distorted,
noisy, and partial patterns \parencite{rumelhart_general_1986}. This
capacity of human memory is known as \textit{content-addressability}: patterns
which are stored in memory are able to be ``looked up'' by themselves or their
parts. Modeling this property is a classical task in computational cognitive
and neuroscience (see \textcites{marr_simple_1971,little_existence_1974,
amari_learning_1972,nakano_associatron-model_1972,stanley_simulation_1976}).
The family of models which implement content-addressability are known
as \textit{associative memory models} (AMs).
A recent revival of interest AMs in machine learning research,
driven by their equivalence with ``attention'' layers in the transformer
architecture \parencites{vaswani_attention_2023, ramsauer_hopfield_2021},
has led to drastic advances in the storage capacity of AMs
\parencites{demircigil_model_2017,krotov_dense_2016,hu_provably_2024}.

The foundational model for modern associative memory research is the
\textit{Hopfield network}
\parencites{hopfield_neural_1982,hopfield_neurons_1984}.
Hopfield networks are single-layer neural networks with full, lateral
connections.
This means that each artificial neuron in the network is connected with every
other neuron, except itself. Patterns that we wish to recall from the network
are stored in the connection weights between every other unit. Hopfield
networks learn new patterns to recall through a simple, biologically plausible
learning rule called the \textit{Activity Product Rule}
\parencite{haykin_neural_2009},
which is a kind of Hebbian update rule \parencite{hebb_organization_1949}.
The appeal of Hebbian update rules is that they are simple, in that they
are totally defined by local interactions between layers in neural networks,
and they are biologically plausible, having been observed in interactions
between biological neurons
\parencites{rolls_mechanisms_2013,bi_synaptic_1998,markram_regulation_1997}.

In spite of their simplicity and biological plausibility, Hopfield networks
are inherently flawed. The number of patterns that Hopfield networks
can store is pitifully small: with estimates in the range of $14\%$ to $15\%$
of the number of neurons in the network
\parencites{hopfield_neural_1982,amit_statistical_1987}.
In order to remedy the gap between the simplicity and plausibility of
Hopfield networks
and their child Dense Associative Memory models, we will theoretically derive
and empirically test the storage capacities of all Hebbian update rules
when used in single-layer neural networks with full lateral connections.

Discovering the limits of Hebbian alternatives to Dense Associative Memories
is not only desirable because of simplicity and biological plausibility
(while those are both laudable goals). For example, the study of
Vector-Symbolic Architectures, used for representing high-level cognitive-tasks
\parencites{smolensky_tensor_1990,plate_holographic_1995,gayler_multiplicative_1998,kelly_encoding_2013},
requires an auto-associative memory ``clean-up memory''. This means that
the expressive capabilities of Vector-Symbolic Architectures are
affected by the capacity of the associative memory used.

In the following we will discuss the problem in more depth. To do
this, we will first
lay out the theoretical motivation: beginning with an introduction to
the literature
in \autoref{sec:background}, covering associative memories in
\autoref{sec:associative-memories},
Hopfield networks in \autoref{sec:hopfield-networks}, their generalization with
Dense Associative Memories in \autoref{sec:dense-associative-memory},
and Hebbian learning
and learning rules in \autoref{sec:hebbian-learning-rules}. We will restate the
problem of Hebbian Dense Associative Memories in \autoref{sec:hebbian-dam}, and
provide a work plan in \autoref{sec:work-plan}. Finally, we will discuss
the timeline for completing this project in \autoref{sec:timeline}.

\section{Background}\label{sec:background}

In order to understand the need for a Hebbian alternative to Dense Associative
Memories, we must first familiarize ourselves with the literature up to this
point. Associative Memory research has a wealth of literature and alternatives.
In order to limit the scope of this research, we limit ourselves only to the
family of models based on Hopfield networks \parencite{hopfield_neural_1982}.

\subsection{Associative Memories}\label{sec:associative-memories}

An associative memory, in particular an \textit{auto}-associative memory,
is a general structure that can be implemented by many mathematical
and computational objects. Broadly speaking, an associative memory is a
tuple of a set of objects $\mathcal X$, called the set of
\textit{traces} or \textit{stored patterns},
and a (typically learned) identity map over the set of stored
patterns, $T: \mathcal X \to \mathcal X$.
More formally,

\begin{definition}[Associative memories]
  An (auto) \textit{associative memory} is a tuple $\langle \mathcal
  X, T \rangle$, where
  \begin{enumerate}[(a)]
    \item $\mathcal X$ is a set of objects, called the
      \textit{pattern} set, or set of \textit{traces}, typically
      high-dimensional vectors; and,
    \item The \textit{recall function}, $T$, maps from the set
      of traces back to
      the set of traces, such that $T(x) \approx x$, for all $x \in
      \mathcal{X}$,
      and $T(\overline x) \approx x$, where $\overline x$ is a
      perturbed, masked, or degraded form of $x \in \mathcal{X}$.
  \end{enumerate}
\end{definition}

We will be assuming throughout that the pattern set is the set of rows
of the \textit{pattern matrix} $\Xi = [\xi^1, \xi^2, \dots, \xi^N]$,
where each \textit{stored pattern} $\xi^i \in \{-1, 1\}^D$. It is sufficient
to specify the pattern matrix, or the sequence of patterns, for an
associative memory,
and it should be assumed that the pattern set is just composed of the
rows of the pattern matrix.
Associative memories are \textit{content-addressable}, as their recall function
is able to recover desired associated patterns from partial information
\parencites{mcclelland_appeal_1986,haykin_neural_2009}. This is in
contrast to memory models which recall information based on arbitrary
associations, like indices into memory locations.

While it is not essential to the most general definition of
associative memories,
we usually desire that the recall function be learned. Furthermore,
associative memories have a maximum capacity of traces that they can store.

\begin{definition}[Retrievability; Capacity]
  Following \textcite{bao_capacity_2022}, let us have an associative
  memory with patterns
  $\xi^1, \xi^2, \dots, \xi^N$, where each element $\xi^i_j$ is
  $-1$ or $1$ with equal likelihood. For any $\delta < 0.5$, we define
  the $\delta$-perturbation of $\xi^i$, $\bar \xi^i$, as
  the $D$-dimensional vector $\xi^i$ with each element flipped with a likelihood
  of $\delta$. Then, we say that the set $\xi^1, \xi^2, \dots, \xi^N$ is
  $(\delta, \varepsilon)$-retrievable if for every $\xi^i$, $i = 1, 2, \dots, N$
  it is such that:
  \begin{equation}
    \mathbb{P}(T(\bar \xi^i) \neq \xi^i) < \varepsilon.
  \end{equation}
  The \textit{capacity} $C$ of the associative memory is the maximum
  cardinality of the pattern set such that all patterns are $(\delta,
  \epsilon)$-retrievable.
\end{definition}

In \textcite{krotov_dense_2016}, capacity is denoted by $N^{\max}$, where
$N$ is the first dimension of the pattern matrix. Associative memory
capacity is affected by the (1) kind of data being stored,
i.e. if the data
has a high pairwise correlation, which leads to ``cross-talk''
\parencite{kohonen_correlation_1988},
(2) the dimensionality of the data, and (3) the implementation of the
recall function $T$.
For example, if the associative memory is a neural network with
tensor weights \parencite{kelly_memory_2017},
then the capacity scales with the number of the weights
\parencite{little_analytic_1978} (expanded
further in \autoref{sec:dense-associative-memory}).

Before Hopfield networks, there were Correlation Matrix associative memories.
So-called, because they relied on dot-product correlations for recall.
\begin{example}[Correlation Matrix associative
  memory]\label{example:correlation}
  Let us have a pattern matrix $\Xi = [\xi^1, \xi^2, \dots, \xi^N]$ of
  vectors sampled from $\{1, -1\}^D$. The Correlation Matrix associative memory
  recall function is of the form:
  \begin{align}
    T(x) &= g \left(\sum^N_{\mu=1} \xi^\mu \left( \sum^D_{i=1}
    \xi^\mu_i x_i \right)\right) \nonumber \\
    &= g \left( \Xi^\top \Xi x  \right),
  \end{align}
  where $g$ is an ``activation function'', typically \textit{signum}
  for binary vectors.
\end{example}

\begin{example}\label{example:argmax-am}
  Like above, let us have a pattern matrix $\Xi = [\xi^1, \xi^2,
  \dots, \xi^N]$ of
  vectors sampled from $\{1, -1\}^D$. Let our recall function $T$ be:
  \begin{equation}
    T(x) = \xi^i,~\text{where}~i = \argmax_{i \in [1, N]}
    [\text{sim}(\xi^i, x)],
  \end{equation}
  where $\text{sim}$ is some similarity function \parencite{kelly_memory_2017},
  e.g. the cosine similarity or Hamming distance.
\end{example}

\begin{example}[Minerva2]\label{example:minerva2}
  Minerva2 \parencite{hintzman_minerva_1984} is an associative memory
  used in cognitive
  science which is about as old as Hopfield networks. As per usual, we have
  binary patterns $\Xi = [\xi^1, \xi^2, \dots, \xi^N]$. The recall function is:
  \begin{equation}
    T(x) = \text{sgn} \left[ \sum^N_{\mu=1} \xi^\mu \left(
    \frac{\sum^D_{i=1} \xi^\mu_i x_i}{\|\xi^\mu\| \|x\|} \right)^3 \right].
  \end{equation}
\end{example}

\subsection{Hopfield Networks}\label{sec:hopfield-networks}

Hopfield networks
\parencites{hopfield_neural_1982,hopfield_neurons_1984} are an associative
memory with deep connections to Example \ref{example:correlation},
except that they
understand their recall function in terms of an \textit{energy} function:

\begin{definition}[Hopfield networks]\label{def:hopfield-nets}
  Hopfield networks are single-layer neural networks of $D$ computational units
  with full, lateral connections. The state of the network is described
  by the state vector $\sigma$ at time $t$, denoted by $\sigma^{(t)}$.
  The \textit{energy} of the network, $E (\sigma^{(t)})$ is:
  \begin{equation}
    E(\sigma^{(t)}) = - \frac{1}{2} \sum^N_{\mu=1} \left(\sum^D_{i=1}
    \xi^\mu_i \sigma^{(t)}_i\right)^2.
  \end{equation}
  Let $\sigma[i := b]$ be the vector $\sigma$, with the $i$'th
  component set to $b$. Then,
  update rule for the state of the network is:
  \begin{equation}
    \sigma^{(t+1)}_i = \text{sgn} \left[ E(\sigma^{(t)}[i : = -1]) -
    E(\sigma^{(t)}[i := 1]) \right].
  \end{equation}
\end{definition}

After setting the initial state at time $t=0$, the network
continuously updates random
elements of the state vector depending upon whether the update lowers
the energy of the
network. The energy function, then, forms a ``landscape'', where
local minima in the space
(ideally) correspond with desired patterns to be stored.

While the use of the energy function seems at odds with our definition
of associative memories, we can align the two by considering a
single-pass variant of the energy-minimization. It can be shown
that updating every element $i$ of the state vector $\sigma^{(t)}$
in a single pass (synchronously) corresponds with the update rule
\parencite{krotov_modern_2025}:
\begin{equation}\label{eq:hopfield-sync}
  T(\sigma) = \text{sgn} \left[ \sum^N_\mu \xi^\mu \left(
  \sum^D_{i=1} \xi^\mu_i \sigma_i \right) \right],
\end{equation}
which is just the update rule of Correlation Matrix associative memories
(see, Example \ref{example:correlation}).

%% NOTE: need to improve the discussion here:
%% 1. linking to the next section
%% 2. what's so cool about hopfield networks? what inspired continued
% work on them?
%% 3. what are some applications of hopfield networks across the
% sciences, make sure
%%    to name drop some cognitive applications
Hopfield networks unfortunately have a pitifully low
storage capacity: the critical capacity $C$ being around $14\%$ to $15\%$
of the number of neurons in the model
\parencites{amit_statistical_1987,hopfield_neural_1982}. As a computational
model of human memory this will not do.  %% why?
Likewise, this is unsuitable for machine learnings tasks involving large
datasets, e.g. large language models. Capacity problems led researchers
to investigate how one can improve the capacity of Hopfield networks,
while maintaining the simplicity and interpretability of energy minimization.

\subsection{Dense Associative Memories}\label{sec:dense-associative-memory}

Dense Associative Memory models are a generalization of Hopfield networks that
maintain energy minimization in recall, but have a dramatically
increased storage capacity \parencite{krotov_dense_2016,demircigil_model_2017}.
Instead of relying on full lateral connections between units in a
single dimension,
Dense Associative Memories either have multiple connections between
computational units, or, increase the number of layers present in the model.
\textcite{krotov_large_2021} notes that this is to be expected, given the
information limits of single neurons.

\begin{definition}[Dense Associative Memory]
  Given binary patterns $\xi^1, \xi^2$, $\dots, \xi^N$ sampled from
  $\{-1, 1\}^D$,
  a \textit{Dense Associative Memory} is characterized by:
  \begin{enumerate}[(i)]
    \item A $D$-dimensional \textit{state vector} at time $t$, $\sigma^{(t)}$
    \item A \textit{polynomial transmission function} $F_n$, where:
      \begin{equation}
        F_n (x) = \frac{1}{n} (x)^n;
      \end{equation}
    \item An \textit{energy function} of the state of the network at time $t$,
      \begin{equation}
        E(\sigma^{(t)}) = \sum^N_{\mu=1} F \left( \sum^D_{i=1}
        \xi^\mu_i \sigma^{(t)}_i \right),
      \end{equation}
    \item An update equation for the state of the network, such that:
      \begin{equation}
        \sigma^{(t+1)}_i = \text{sgn} \left[ E(\sigma^{(t)}[i := -1])
        - E(\sigma^{(t)}[i := 1]) \right].
      \end{equation}
  \end{enumerate}
\end{definition}

Like Hopfield networks, the state vector update rule is typically
performed asynchronously.
However, we can define a synchronous update rule which makes the
Dense Associative
Memory an Associative Memory with update rule:
\begin{equation}
  T(\sigma) = \text{sgn} \left[\sum^N_{\mu=1} \xi^\mu F_n' \left(
  \sum^D_{i=1} \xi^\mu_i \sigma_i \right)\right],
\end{equation}
where $F_n'$ is the derivative of $F_n$. From this definition we immediately
see how this framework generalizes Hopfield networks.

\begin{example}
  Hopfield networks in Definition \ref{def:hopfield-nets} are Dense
  Associative Memories
  with polynomial transmission function $F_2$.
\end{example}

\begin{example}
  Minerva2 (from Example \ref{example:minerva2}) are roughly Dense
  Associative Networks,
  assuming that all patterns $\xi^1, \xi^2, \dots, \xi^N$ and query
  patterns are normalized,
  and with polynomial transmission function $F_4$.
\end{example}

Knowing this, one can now see
the relation between the state update rule of Hopfield networks in
Definition \ref{def:hopfield-nets} and \autoref{eq:hopfield-sync}.

Dense Associative Memories improve the storage capacity of Hopfield networks
in that the capacity $C$ of a Dense Associative Memory, with
polynomial function $F_n$,
and dimensionality of patterns $D$ is:
\begin{equation}
  C \propto D^{n-1}
\end{equation}
\parencites{krotov_dense_2016,demircigil_model_2017,bao_capacity_2022}.
The increased capacity comes from the ``steeper'' energy basins
around minima corresponding to patterns stored in the network, which
is brought about by the exponentiation of the dot product similarity
between the stored patterns and the query pattern (for discussion, see
\textcite{kelly_memory_2017}). Memory capacity for Dense Associative
Memories can also be increased by making more domain sensitive
similarity functions \parencite{millidge_universal_2022},
e.g. with kernel estimation \parencite{hu_provably_2024,wu_uniform_2024}.

In spite of the enormous gains in capacity using non-linear
transmission functions, Dense Associative Memories lose the ability
to be interpreted as simple one-layer networks with lateral connections
and trained via Hebbian update rules
\parencites{krotov_large_2021,mcalister_sequential_2025}.
On the one hand, this is to their benefit: a continuous variant of
Dense Associative
Memory called \textit{Modern Hopfield Networks} has been shown to be
equivalent to multi-head attention in Transformer neural networks
\parencites{ramsauer_hopfield_2021,vaswani_attention_2023}. Likewise,
the framework behind Dense Associative Memories has been developed for
modular neural network architectures \parencite{krotov_hierarchical_2021},
and has been used to define an \textit{Energy Based Transformer}
\parencite{hoover_energy_2023}, as well as contribute to the interpretability
of Diffusion Models \parencite{pham_memorization_2025}.

But the cost is their simplicity. Correlation matrix memories
trained with Hebbian update rules mimic observed phenomena in
biological neural networks; meaning that they perform computations
which are in principle achievable by biological systems.

\subsection{Hebbian Learning Rules}\label{sec:hebbian-learning-rules}

%% OUTLINE
Hopfield networks learn with the Activity Product Rule
\parencite{haykin_neural_2009}.
Talk about biological evidence for Hebbian learning rules. Stress simplicity.

The closest investigation to this proposed analysis is
\textcite{lansner_benchmarking_2025}, which compares several
extant Hebbian learning rules in the literature: namely, the Willshaw
learning rule, the Activity Product Rule, the Covariance Learning rule,
the Presynaptic Covariance learning rule, the Bayesian Confidence Propagation
learning rule, and the Bayesian Optimal Memory learning rule.

This work serves as an important comparison work between the theoretical
results that we wish to derive.

\section{Hebbian Dense Associative Memory}\label{sec:hebbian-dam}

%% OUTLINE
Outline of the project. Hopfield networks which, instead of
manipulating a polynomial
function, rather manipulate how patterns are stored to begin with.

\subsection{Work Plan}\label{sec:work-plan}

%% OUTLINE
Two main components of the project: theoretical and experimental. Theoretical
involves proving some theorems about the minimum and maximum capacity of
Hopfield networks with different Hebbian learning rules. Experimental work
means implementing the different networks and testing them on a task. Suggest
testing them on classification of degraded inputs.

\section{Timeline}\label{sec:timeline}

%% OUTLINE
Theoretical work should be done by Christmas. Experimental work likewise
requires only a couple of months. Data science certification requires emphasis
here on large-scale projects. So, I would like to rigorously examine
each model on a variety of tasks. One to two papers arises from this project:
one talking about the theory of Hebbian DAM, and the other part including
the experimental work. It would be better for a single paper, but if that
runs too long, then experimental work can be a part 2.

\printbibliography
\end{document}
