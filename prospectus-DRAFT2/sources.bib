
@article{mewhort_extralist-feature_2000,
	title = {The Extralist-Feature Effect: Evidence Against Item Matching in Short-Term Recognition Memory},
	volume = {129},
	pages = {262--284},
	number = {2},
	journaltitle = {Journal of Experimental Psychology},
	author = {Mewhort, D J K and Johns, Elizabeth E},
	date = {2000},
	langid = {english},
	file = {PDF:/home/connorh/Zotero/storage/4LMDBFYQ/Mewhort and Johns - The Extralist-Feature Effect Evidence Against Item Matching in Short-Term Recognition Memory.pdf:application/pdf},
}

@article{anderson_retrieval_1974,
	title = {Retrieval of propositional information from long-term memory},
	volume = {6},
	rights = {https://www.elsevier.com/tdm/userlicense/1.0/},
	issn = {00100285},
	url = {https://linkinghub.elsevier.com/retrieve/pii/0010028574900218},
	doi = {10.1016/0010-0285(74)90021-8},
	pages = {451--474},
	number = {4},
	journaltitle = {Cognitive Psychology},
	shortjournal = {Cognitive Psychology},
	author = {Anderson, John Robert},
	urldate = {2025-08-27},
	date = {1974-10},
	langid = {english},
	file = {PDF:/home/connorh/Zotero/storage/2VV8BGEH/Anderson - 1974 - Retrieval of propositional information from long-term memory.pdf:application/pdf},
}

@article{osth_running_nodate,
	title = {Running head: {SEQUENTIAL} {SAMPLING} {MODELS} {MEMORY}},
	abstract = {In the domain of memory research, there are strong relationships between the accuracy and response times ({RT}) for choices in tasks such as recognition memory and recall. The strongest uniﬁcation of these variables to come out of cognitive psychology is in the sequential sampling model framework, in which evidence accumulates in a noisy fashion until a threshold for a given response is reached. In this chapter, we provide an overview of these models within the memory domain. We begin with the classical diﬀusion model along with the motivations for the "full" diﬀusion model, which generalizes the classical diﬀusion model to include cross-trial variability in drift rates and starting points. We further describe how such models can produce interpretable parameter estimates for testing psychological theory. In addition, we summarize the connections between diﬀusion models and signal detection theory, extensions to conﬁdence, recall, and continuous responses, and how such models can serve as back-end decision models for theories of retrieval.},
	author = {Osth, Adam F and Zhou, Jason and Chen, Haomin and Sun, Jie},
	langid = {english},
	file = {PDF:/home/connorh/Zotero/storage/XRNXKF42/Osth et al. - Running head SEQUENTIAL SAMPLING MODELS MEMORY.pdf:application/pdf},
}

@article{cox_dynamic_2024,
	title = {Dynamic retrieval of events and associations from memory: An integrated account of item and associative recognition.},
	volume = {131},
	issn = {1939-1471, 0033-295X},
	url = {https://doi.apa.org/doi/10.1037/rev0000486},
	doi = {10.1037/rev0000486},
	shorttitle = {Dynamic retrieval of events and associations from memory},
	abstract = {Memory theories distinguish between item and associative information, which are engaged by different tasks: item recognition uses item information to decide whether an event occurred in a particular context; associative recognition uses associative information to decide whether two events occurred together. Associative recognition is slower and less accurate than item recognition, suggesting that item and associative information may be represented in different forms and retrieved using different processes. Instead, I show how a dynamic model (Cox \& Criss, 2020; Cox \& Shiffrin, 2017) accounts for accuracy and response time distributions in both item and associative recognition with the same set of representations and processes. Item and associative information are both represented as vectors of features. Item and associative recognition both depend on comparing traces in memory with probes of memory in which item and associative features gradually accumulate. Associative features are slower to accumulate, but largely because they emerge from conjunctions of already-accumulated item features. I apply the model to data from 453 participants who each performed item and associative recognition following identical study conditions (Cox et al., 2018). Comparisons among restricted versions of the model show that its account of associative feature formation, coupled with limits on the rate at which features accumulate from multiple items, explains how and why the dynamics of associative recognition differ from those of item recognition even while both tasks rely on the same underlying representations.},
	pages = {1297--1336},
	number = {6},
	journaltitle = {Psychological Review},
	shortjournal = {Psychological Review},
	author = {Cox, Gregory E.},
	urldate = {2025-08-27},
	date = {2024-11},
	langid = {english},
	file = {PDF:/home/connorh/Zotero/storage/HNGU9R5B/Cox - 2024 - Dynamic retrieval of events and associations from memory An integrated account of item and associat.pdf:application/pdf},
}

@article{friston_free-energy_2010,
	title = {The free-energy principle: a unified brain theory?},
	volume = {11},
	issn = {1471-003X, 1471-0048},
	url = {https://www.nature.com/articles/nrn2787},
	doi = {10.1038/nrn2787},
	shorttitle = {The free-energy principle},
	pages = {127--138},
	number = {2},
	journaltitle = {Nature Reviews Neuroscience},
	shortjournal = {Nat Rev Neurosci},
	author = {Friston, Karl},
	urldate = {2025-08-19},
	date = {2010-02},
	langid = {english},
	file = {PDF:/home/connorh/Zotero/storage/V26EUBNQ/Friston - 2010 - The free-energy principle a unified brain theory.pdf:application/pdf},
}

@misc{millidge_predictive_2022,
	title = {Predictive Coding: a Theoretical and Experimental Review},
	url = {http://arxiv.org/abs/2107.12979},
	doi = {10.48550/arXiv.2107.12979},
	shorttitle = {Predictive Coding},
	abstract = {Predictive coding offers a potentially unifying account of cortical function – postulating that the core function of the brain is to minimize prediction errors with respect to a generative model of the world. The theory is closely related to the Bayesian brain framework and, over the last two decades, has gained substantial inﬂuence in the ﬁelds of theoretical and cognitive neuroscience. A large body of research has arisen based on both empirically testing improved and extended theoretical and mathematical models of predictive coding, as well as in evaluating their potential biological plausibility for implementation in the brain and the concrete neurophysiological and psychological predictions made by the theory. Despite this enduring popularity, however, no comprehensive review of predictive coding theory, and especially of recent developments in this ﬁeld, exists. Here, we provide a comprehensive review both of the core mathematical structure and logic of predictive coding, thus complementing recent tutorials in the literature (Bogacz, 2017; Buckley, Kim, {McGregor}, \& Seth, 2017). We also review a wide range of classic and recent work within the framework, ranging from the neurobiologically realistic microcircuits that could implement predictive coding, to the close relationship between predictive coding and the widely-used backpropagation of error algorithm, as well as surveying the close relationships between predictive coding and modern machine learning techniques.},
	number = {{arXiv}:2107.12979},
	publisher = {{arXiv}},
	author = {Millidge, Beren and Seth, Anil and Buckley, Christopher L.},
	urldate = {2025-08-19},
	date = {2022-07-12},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2107.12979 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Neural and Evolutionary Computing, Quantitative Biology - Neurons and Cognition},
	file = {PDF:/home/connorh/Zotero/storage/SQQPR297/Millidge et al. - 2022 - Predictive Coding a Theoretical and Experimental Review.pdf:application/pdf},
}

@misc{vaswani_attention_2023,
	title = {Attention Is All You Need},
	url = {http://arxiv.org/abs/1706.03762},
	doi = {10.48550/arXiv.1706.03762},
	abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 {BLEU} on the {WMT} 2014 Englishto-German translation task, improving over the existing best results, including ensembles, by over 2 {BLEU}. On the {WMT} 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art {BLEU} score of 41.8 after training for 3.5 days on eight {GPUs}, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
	number = {{arXiv}:1706.03762},
	publisher = {{arXiv}},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
	urldate = {2025-08-17},
	date = {2023-08-02},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1706.03762 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {PDF:/home/connorh/Zotero/storage/KAA8QA8A/Vaswani et al. - 2023 - Attention Is All You Need.pdf:application/pdf},
}

@misc{ramsauer_hopfield_2021,
	title = {Hopfield Networks is All You Need},
	url = {http://arxiv.org/abs/2008.02217},
	doi = {10.48550/arXiv.2008.02217},
	abstract = {We introduce a modern Hopfield network with continuous states and a corresponding update rule. The new Hopfield network can store exponentially (with the dimension of the associative space) many patterns, retrieves the pattern with one update, and has exponentially small retrieval errors. It has three types of energy minima (fixed points of the update): (1) global fixed point averaging over all patterns, (2) metastable states averaging over a subset of patterns, and (3) fixed points which store a single pattern. The new update rule is equivalent to the attention mechanism used in transformers. This equivalence enables a characterization of the heads of transformer models. These heads perform in the first layers preferably global averaging and in higher layers partial averaging via metastable states. The new modern Hopfield network can be integrated into deep learning architectures as layers to allow the storage of and access to raw input data, intermediate results, or learned prototypes. These Hopfield layers enable new ways of deep learning, beyond fully-connected, convolutional, or recurrent networks, and provide pooling, memory, association, and attention mechanisms. We demonstrate the broad applicability of the Hopfield layers across various domains. Hopfield layers improved state-of-the-art on three out of four considered multiple instance learning problems as well as on immune repertoire classification with several hundreds of thousands of instances. On the {UCI} benchmark collections of small classification tasks, where deep learning methods typically struggle, Hopfield layers yielded a new state-of-the-art when compared to different machine learning methods. Finally, Hopfield layers achieved state-of-the-art on two drug design datasets. The implementation is available at: https://github.com/ml-jku/hopfield-layers},
	number = {{arXiv}:2008.02217},
	publisher = {{arXiv}},
	author = {Ramsauer, Hubert and Schäfl, Bernhard and Lehner, Johannes and Seidl, Philipp and Widrich, Michael and Adler, Thomas and Gruber, Lukas and Holzleitner, Markus and Pavlović, Milena and Sandve, Geir Kjetil and Greiff, Victor and Kreil, David and Kopp, Michael and Klambauer, Günter and Brandstetter, Johannes and Hochreiter, Sepp},
	urldate = {2025-08-17},
	date = {2021-04-28},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2008.02217 [cs]},
	keywords = {Computer Science - Neural and Evolutionary Computing, Computer Science - Computation and Language, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {PDF:/home/connorh/Zotero/storage/2H7ZEF9U/Ramsauer et al. - 2021 - Hopfield Networks is All You Need.pdf:application/pdf},
}

@misc{saha_end--end_2023,
	title = {End-to-end Differentiable Clustering with Associative Memories},
	url = {http://arxiv.org/abs/2306.03209},
	doi = {10.48550/arXiv.2306.03209},
	abstract = {Clustering is a widely used unsupervised learning technique involving an intensive discrete optimization problem. Associative Memory models or {AMs} are differentiable neural networks defining a recursive dynamical system, which have been integrated with various deep learning architectures. We uncover a novel connection between the {AM} dynamics and the inherent discrete assignment necessary in clustering to propose a novel unconstrained continuous relaxation of the discrete clustering problem, enabling end-to-end differentiable clustering with {AM}, dubbed {ClAM}. Leveraging the pattern completion ability of {AMs}, we further develop a novel self-supervised clustering loss. Our evaluations on varied datasets demonstrate that {ClAM} benefits from the selfsupervision, and significantly improves upon both the traditional Lloyd’s k-means algorithm, and more recent continuous clustering relaxations (by upto 60\% in terms of the Silhouette Coefficient).},
	number = {{arXiv}:2306.03209},
	publisher = {{arXiv}},
	author = {Saha, Bishwajit and Krotov, Dmitry and Zaki, Mohammed J. and Ram, Parikshit},
	urldate = {2025-08-17},
	date = {2023-06-05},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2306.03209 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {PDF:/home/connorh/Zotero/storage/R9Z8NLPD/Saha et al. - 2023 - End-to-end Differentiable Clustering with Associative Memories.pdf:application/pdf},
}

@misc{krotov_dense_2016,
	title = {Dense Associative Memory for Pattern Recognition},
	url = {http://arxiv.org/abs/1606.01164},
	doi = {10.48550/arXiv.1606.01164},
	abstract = {A model of associative memory is studied, which stores and reliably retrieves many more patterns than the number of neurons in the network. We propose a simple duality between this dense associative memory and neural networks commonly used in deep learning. On the associative memory side of this duality, a family of models that smoothly interpolates between two limiting cases can be constructed. One limit is referred to as the feature-matching mode of pattern recognition, and the other one as the prototype regime. On the deep learning side of the duality, this family corresponds to feedforward neural networks with one hidden layer and various activation functions, which transmit the activities of the visible neurons to the hidden layer. This family of activation functions includes logistics, rectiﬁed linear units, and rectiﬁed polynomials of higher degrees. The proposed duality makes it possible to apply energy-based intuition from associative memory to analyze computational properties of neural networks with unusual activation functions – the higher rectiﬁed polynomials which until now have not been used in deep learning. The utility of the dense memories is illustrated for two test cases: the logical gate {XOR} and the recognition of handwritten digits from the {MNIST} data set.},
	number = {{arXiv}:1606.01164},
	publisher = {{arXiv}},
	author = {Krotov, Dmitry and Hopfield, John J.},
	urldate = {2025-08-17},
	date = {2016-09-27},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1606.01164 [cs]},
	keywords = {Computer Science - Neural and Evolutionary Computing, Quantitative Biology - Neurons and Cognition, Computer Science - Machine Learning, Statistics - Machine Learning, Condensed Matter - Disordered Systems and Neural Networks},
	file = {dense_associative_memories_for_pattern_recognition:/home/connorh/Zotero/storage/DCDZIQXM/dense_associative_memories_for_pattern_recognition.pdf:application/pdf;PDF:/home/connorh/Zotero/storage/IQFDFM8J/dense_associative_memories_for_pattern_recognition.pdf:application/pdf},
}

@article{demircigil_model_2017,
	title = {On a model of associative memory with huge storage capacity},
	volume = {168},
	issn = {0022-4715, 1572-9613},
	url = {http://arxiv.org/abs/1702.01929},
	doi = {10.1007/s10955-017-1806-y},
	abstract = {In [7] Krotov and Hopﬁeld suggest a generalized version of the wellknown Hopﬁeld model of associative memory. In their version they consider a polynomial interaction function and claim that this increases the storage capacity of the model. We prove this claim and take the ”limit” as the degree of the polynomial becomes inﬁnite, i.e. an exponential interaction function. With this interaction we prove that model has an exponential storage capacity in the number of neurons, yet the basins of attraction are almost as large as in the standard Hopﬁeld model.},
	pages = {288--299},
	number = {2},
	journaltitle = {Journal of Statistical Physics},
	shortjournal = {J Stat Phys},
	author = {Demircigil, Mete and Heusel, Judith and Löwe, Matthias and Upgang, Sven and Vermet, Franck},
	urldate = {2025-08-17},
	date = {2017-07},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1702.01929 [math]},
	keywords = {Mathematics - Probability},
	file = {PDF:/home/connorh/Zotero/storage/AZ7FKLLD/Demircigil et al. - 2017 - On a model of associative memory with huge storage capacity.pdf:application/pdf},
}

@article{zhao_bridging_nodate,
	title = {Bridging the Gap Between Hyperdimensional Computing and Kernel Methods via the Nyström Method},
	abstract = {Hyperdimensional computing ({HDC}) is an approach from the cognitive science literature for solving information processing tasks using data represented as high-dimensional random vectors. The technique has a rigorous mathematical backing, and is easy to implement in energy-efficient and highly parallel hardware like {FPGAs} and “processing-in-memory” architectures. The effectiveness of {HDC} in machine learning largely depends on how raw data is mapped to highdimensional space. In this work, we propose {NysHD}, a new method for constructing this mapping that is based on the Nystr¨om method from the literature on kernel approximation. Our approach provides a simple recipe to turn any user-defined positive-semidefinite similarity function into an equivalent mapping in {HDC}. There is a vast literature on the design of such functions for learning problems. Our approach provides a mechanism to import them into the {HDC} setting, expanding the types of problems that can be tackled using {HDC}. Empirical evaluation against existing {HDC} encoding methods shows that {NysHD} can achieve, on average, 11\% and 17\% better classification accuracy on graph and string datasets respectively.},
	author = {Zhao, Quanling and Thomas, Anthony Hitchcock and Brin, Ari and Yu, Xiaofan and Rosing, Tajana},
	langid = {english},
	file = {PDF:/home/connorh/Zotero/storage/G357SL5M/Zhao et al. - Bridging the Gap Between Hyperdimensional Computing and Kernel Methods via the Nyström Method.pdf:application/pdf},
}

@article{laird_standard_2017,
	title = {A Standard Model of the Mind: Toward a Common Computational Framework Across Artificial Intelligence, Cognitive Science, Neuroscience, and Robotics},
	volume = {38},
	rights = {http://onlinelibrary.wiley.com/{termsAndConditions}\#am},
	issn = {0738-4602, 2371-9621},
	url = {https://onlinelibrary.wiley.com/doi/10.1609/aimag.v38i4.2744},
	doi = {10.1609/aimag.v38i4.2744},
	shorttitle = {A Standard Model of the Mind},
	abstract = {A standard model captures a community consensus over a coherent region of science, serving as a cumulative reference point for the field that can provide guidance for both research and applications, while also focusing efforts to extend or revise it. Here we propose developing such a model for humanlike minds, computational entities whose structures and processes are substantially similar to those found in human cognition. Our hypothesis is that cognitive architectures provide the appropriate computational abstraction for defining a standard model, although the standard model is not itself such an architecture. The proposed standard model began as an initial consensus at the 2013 {AAAI} Fall Symposium on Integrated Cognition, but is extended here through a synthesis across three existing cognitive architectures: {ACT}‐R, Sigma, and Soar. The resulting standard model spans key aspects of structure and processing, memory and content, learning, and perception and motor, and highlights loci of architectural agreement as well as disagreement with the consensus while identifying potential areas of remaining incompleteness. The hope is that this work will provide an important step toward engaging the broader community in further development of the standard model of the mind.},
	pages = {13--26},
	number = {4},
	journaltitle = {{AI} Magazine},
	shortjournal = {{AI} Magazine},
	author = {Laird, John E. and Lebiere, Christian and Rosenbloom, Paul S.},
	urldate = {2025-08-17},
	date = {2017-12},
	langid = {english},
	file = {PDF:/home/connorh/Zotero/storage/M6QKKNVJ/Laird et al. - 2017 - A Standard Model of the Mind Toward a Common Computational Framework Across Artificial Intelligence.pdf:application/pdf},
}

@article{levy_lateral_nodate,
	title = {“Lateral Inhibition” in a Fully Distributed Connectionist Architecture},
	abstract = {We present a fully distributed connectionist architecture supporting lateral inhibition / winner-takes all competition. All items (individuals, relations, and structures) are represented by high-dimensional distributed vectors, and (multi)sets of items as the sum of such vectors. The architecture uses a neurally plausible permutation circuit to support a multiset intersection operation without decomposing the summed vector into its constituent items or requiring more hardware for more complex representations. Iterating this operation produces a vector in which an initially slightly favored item comes to dominate the others. This result (1) challenges the view that lateral inhibition calls for localist representation; and (2) points toward a neural implementation where more complex representations do not require more complex hardware.},
	author = {Levy, Simon D and Gayler, Ross W},
	langid = {english},
	file = {PDF:/home/connorh/Zotero/storage/N8YD3DRZ/Levy and Gayler - “Lateral Inhibition” in a Fully Distributed Connectionist Architecture.pdf:application/pdf},
}

@inproceedings{ororbia_maze_2023,
	location = {Cham},
	title = {Maze Learning Using a Hyperdimensional Predictive Processing Cognitive Architecture},
	isbn = {978-3-031-19907-3},
	abstract = {We present the {COGnitive} Neural {GENerative} system ({CogNGen}), a cognitive architecture that combines two neurobiologically-plausible, computational models: predictive processing and hyperdimensional/vector-symbolic models. We draw inspiration from architectures such as {ACT}-R and Spaun/Nengo. {CogNGen} is in broad agreement with these, providing a level of detail between {ACT}-R’s high-level symbolic description of human cognition and Spaun’s low-level neurobiological description, furthermore creating the groundwork for designing agents that learn continually from diverse tasks and model human performance at larger scales than what is possible with current systems. We test {CogNGen} on four maze-learning tasks, including those that test memory and planning, and find that {CogNGen} matches performance of deep reinforcement learning models and exceeds on a task designed to test memory.},
	pages = {321--331},
	booktitle = {Artificial General Intelligence},
	publisher = {Springer International Publishing},
	author = {Ororbia, Alexander G. and Kelly, M. Alex},
	editor = {Goertzel, Ben and Iklé, Matt and Potapov, Alexey and Ponomaryov, Denis},
	date = {2023},
}

@article{hintzman_minerva_1984,
	title = {{MINERVA} 2: A simulation model of human memory},
	volume = {16},
	rights = {http://www.springer.com/tdm},
	issn = {0743-3808, 1532-5970},
	url = {http://link.springer.com/10.3758/BF03202365},
	doi = {10.3758/BF03202365},
	shorttitle = {{MINERVA} 2},
	pages = {96--101},
	number = {2},
	journaltitle = {Behavior Research Methods, Instruments, \&amp Computers},
	shortjournal = {Behavior Research Methods, Instruments, \& Computers},
	author = {Hintzman, Douglas L.},
	urldate = {2025-08-17},
	date = {1984-03},
	langid = {english},
	file = {PDF:/home/connorh/Zotero/storage/4YII7SP2/Hintzman - 1984 - MINERVA 2 A simulation model of human memory.pdf:application/pdf},
}

@article{kelly_holographic_2020,
	title = {Holographic Declarative Memory: Distributional Semantics as the Architecture of Memory},
	volume = {44},
	issn = {0364-0213, 1551-6709},
	url = {https://onlinelibrary.wiley.com/doi/10.1111/cogs.12904},
	doi = {10.1111/cogs.12904},
	shorttitle = {Holographic Declarative Memory},
	abstract = {We demonstrate that the key components of cognitive architectures (declarative and procedural memory) and their key capabilities (learning, memory retrieval, probability judgment, and utility estimation) can be implemented as algebraic operations on vectors and tensors in a high-dimensional space using a distributional semantics model. High-dimensional vector spaces underlie the success of modern machine learning techniques based on deep learning. However, while neural networks have an impressive ability to process data to ﬁnd patterns, they do not typically model high-level cognition, and it is often unclear how they work. Symbolic cognitive architectures can capture the complexities of high-level cognition and provide human-readable, explainable models, but scale poorly to naturalistic, non-symbolic, or big data. Vector-symbolic architectures, where symbols are represented as vectors, bridge the gap between the two approaches. We posit that cognitive architectures, if implemented in a vector-space model, represent a useful, explanatory model of the internal representations of otherwise opaque neural architectures. Our proposed model, Holographic Declarative Memory ({HDM}), is a vector-space model based on distributional semantics. {HDM} accounts for primacy and recency effects in free recall, the fan effect in recognition, probability judgments, and human performance on an iterated decision task. {HDM} provides a ﬂexible, scalable alternative to symbolic cognitive architectures at a level of description that bridges symbolic, quantum, and neural models of cognition.},
	pages = {e12904},
	number = {11},
	journaltitle = {Cognitive Science},
	shortjournal = {Cognitive Science},
	author = {Kelly, Mary Alexandria and Arora, Nipun and West, Robert L. and Reitter, David},
	urldate = {2025-08-17},
	date = {2020-11},
	langid = {english},
	file = {PDF:/home/connorh/Zotero/storage/FYG8VTVE/Kelly et al. - 2020 - Holographic Declarative Memory Distributional Semantics as the Architecture of Memory.pdf:application/pdf;PDF:/home/connorh/Zotero/storage/Q6ENJB9J/Kelly et al. - 2020 - Holographic Declarative Memory Distributional Semantics as the Architecture of Memory.pdf:application/pdf},
}

@article{kelly_memory_2017,
	title = {The memory tesseract: Mathematical equivalence between composite and separate storage memory models},
	volume = {77},
	issn = {00222496},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0022249616301122},
	doi = {10.1016/j.jmp.2016.10.006},
	shorttitle = {The memory tesseract},
	abstract = {Computational memory models can explain the behaviour of human memory in diverse experimental paradigms. But research has produced a profusion of competing models, and, as different models focus on different phenomena, there is no best model. However, by examining commonalities among models, we can move towards theoretical unification. Computational memory models can be grouped into composite and separate storage models. We prove that {MINERVA} 2, a separate storage model of long-term memory, is mathematically equivalent to composite storage memory implemented as a fourth order tensor, and approximately equivalent to a fourth-order tensor compressed into a holographic vector. Building of these demonstrations, we show that {MINERVA} 2 and related separate storage models can be implemented in neurons. Our work clarifies the relationship between composite and separate storage models of memory, and thereby moves memory models a step closer to theoretical unification.},
	pages = {142--155},
	journaltitle = {Journal of Mathematical Psychology},
	shortjournal = {Journal of Mathematical Psychology},
	author = {Kelly, Mary Alexandria and Mewhort, D.J.K. and West, Robert L.},
	urldate = {2025-08-17},
	date = {2017-04},
	langid = {english},
	file = {PDF:/home/connorh/Zotero/storage/GLZ9F3K5/Kelly et al. - 2017 - The memory tesseract Mathematical equivalence between composite and separate storage memory models.pdf:application/pdf;PDF:/home/connorh/Zotero/storage/QM7VB5KK/Kelly et al. - 2017 - The memory tesseract Mathematical equivalence between composite and separate storage memory models.pdf:application/pdf},
}

@incollection{kohonen_correlation_1988,
	title = {Correlation matrix memories},
	isbn = {978-0-262-26713-7},
	url = {https://direct.mit.edu/books/book/5431/chapter/4468693/1972-Teuvo-Kohonen-Correlation-matrix-memories},
	shorttitle = {(1972) Teuvo Kohonen, "Correlation matrix memories," {IEEE} Transactions on Computers C-21},
	pages = {174--180},
	booktitle = {Neurocomputing, Volume 1},
	publisher = {The {MIT} Press},
	author = {Kohonen, Teuvo},
	editor = {Anderson, James A. and Rosenfeld, Edward},
	urldate = {2025-09-16},
	date = {1988-04-07},
	langid = {english},
	doi = {10.7551/mitpress/4943.003.0075},
	file = {PDF:/home/connorh/Zotero/storage/MGRGLU2W/Kohonen - 1988 - (1972) Teuvo Kohonen, Correlation matrix memories, IEEE Transactions on Computers C-21 353-359.pdf:application/pdf},
}

@article{lecun_mnist_2010,
	title = {{MNIST} handwritten digit database},
	volume = {2},
	url = {http://yann.lecun.com/exdb/mnist},
	journaltitle = {{ATT} Labs},
	author = {{LeCun}, Yann and Cortes, Corinna and Burges, {CJ}},
	date = {2010},
}

@book{marx_eighteenth_1852,
	location = {New York},
	title = {The Eighteenth Brumaire of Louis Bonaparte},
	isbn = {978-0-7178-0056-8},
	url = {https://www.marxists.org/archive/marx/works/1852/18th-brumaire/},
	publisher = {Die Revolution},
	author = {Marx, Karl},
	urldate = {2025-09-23},
	date = {1852},
}

@article{willshaw_non-holographic_1969,
	title = {Non-Holographic Associative Memory},
	volume = {222},
	rights = {http://www.springer.com/tdm},
	issn = {0028-0836, 1476-4687},
	url = {https://www.nature.com/articles/222960a0},
	doi = {10.1038/222960a0},
	pages = {960--962},
	number = {5197},
	journaltitle = {Nature},
	shortjournal = {Nature},
	author = {Willshaw, D. J. and Buneman, O. P. and Longuet-Higgins, H. C.},
	urldate = {2025-09-22},
	date = {1969-06},
	langid = {english},
	file = {PDF:/home/connorh/Zotero/storage/RQ8TC2RY/Willshaw et al. - 1969 - Non-Holographic Associative Memory.pdf:application/pdf},
}

@incollection{wess_holographic_1978,
	title = {A holographic brain model for associative memory},
	volume = {3},
	isbn = {0-470-26371-7},
	url = {http://geca.area.ge.cnr.it/files/72373.pdf},
	volumes = {4},
	pages = {580--580},
	booktitle = {Progress in Cybernetics and Systems Research},
	publisher = {Hemisphere Publishing Company},
	author = {Wess, O. and Röder, U.},
	urldate = {2025-09-22},
	date = {1978},
	langid = {english},
	file = {PDF:/home/connorh/Zotero/storage/V2A6Z98H/a_holographic_brain_model_for_associative_memory.pdf:application/pdf},
}

@article{wess_holographic_1977,
	title = {A holographic model for associative memory chains},
	volume = {27},
	rights = {http://www.springer.com/tdm},
	issn = {0340-1200, 1432-0770},
	url = {http://link.springer.com/10.1007/BF00337260},
	doi = {10.1007/BF00337260},
	pages = {89--98},
	number = {2},
	journaltitle = {Biological Cybernetics},
	shortjournal = {Biol. Cybern.},
	author = {Wess, O. and Röder, U.},
	urldate = {2025-09-22},
	date = {1977},
	langid = {english},
	file = {PDF:/home/connorh/Zotero/storage/9268RAW3/Wess and R�der - 1977 - A holographic model for associative memory chains.pdf:application/pdf},
}

@article{eich_composite_1982,
	title = {A composite holographic associative recall model.},
	volume = {89},
	issn = {1939-1471, 0033-295X},
	url = {https://doi.apa.org/doi/10.1037/0033-295X.89.6.627},
	doi = {10.1037/0033-295X.89.6.627},
	pages = {627--661},
	number = {6},
	journaltitle = {Psychological Review},
	shortjournal = {Psychological Review},
	author = {Eich, Janet M.},
	urldate = {2025-09-22},
	date = {1982-11},
	langid = {english},
	file = {PDF:/home/connorh/Zotero/storage/9P6U9YC5/Eich - 1982 - A composite holographic associative recall model..pdf:application/pdf},
}

@article{owechko_nonlinear_1989,
	title = {Nonlinear holographic associative memories},
	volume = {25},
	rights = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/{IEEE}.html},
	issn = {00189197},
	url = {http://ieeexplore.ieee.org/document/18575/},
	doi = {10.1109/3.18575},
	abstract = {Recent progress in the field of holographic optical associative memories is reviewed. The addition of nonlinear optical feedback to holographic memories using phase con,jugation techniques has resulted in a new class of nonlinear holographic associative memories. These systenis perform associations hetween input patterns and stored patterns as in classical holograph). Unlike classical holography, however, external nonlinearities are used to make decisions and select between competing alternatives, perform error correction, and increase storage capacity. Such optical associative processors may have applications in future implementations of optical neural networks.},
	pages = {619--634},
	number = {3},
	journaltitle = {{IEEE} Journal of Quantum Electronics},
	shortjournal = {{IEEE} J. Quantum Electron.},
	author = {Owechko, Y.},
	urldate = {2025-09-22},
	date = {1989-03},
	langid = {english},
	file = {PDF:/home/connorh/Zotero/storage/9E6IULEX/Owechko - 1989 - Nonlinear holographic associative memories.pdf:application/pdf},
}

@article{franklin_memory_2015,
	title = {Memory as a hologram: An analysis of learning and recall.},
	volume = {69},
	issn = {1878-7290, 1196-1961},
	url = {https://doi.apa.org/doi/10.1037/cep0000035},
	doi = {10.1037/cep0000035},
	shorttitle = {Memory as a hologram},
	abstract = {We present a holographic theory of human memory. According to the theory, a subject’s vocabulary resides in a dynamic distributed representation—a hologram. Studying or recalling a word alters both the existing representation of that word in the hologram and all words associated with it. Recall is always prompted by a recall cue (either a start instruction or the word just recalled). Order of report is a joint function of the item and associative information residing in the hologram at the time the report is made. We apply the model to archival data involving simple free recall, learning in multitrial free recall, simple serial recall, and learning in multitrial serial recall. The model captures accuracy and order of report in both free and serial recall. It also captures learning and subjective organisation in multitrial free recall. We offer the model as an alternative to the short- and long-term account of memory postulated in the modal model.},
	pages = {115--135},
	number = {1},
	journaltitle = {Canadian Journal of Experimental Psychology / Revue canadienne de psychologie expérimentale},
	shortjournal = {Canadian Journal of Experimental Psychology / Revue canadienne de psychologie expérimentale},
	author = {Franklin, Donald R. J. and Mewhort, D. J. K.},
	urldate = {2025-09-22},
	date = {2015-03},
	langid = {english},
	file = {PDF:/home/connorh/Zotero/storage/2TJN7H9U/Franklin and Mewhort - 2015 - Memory as a hologram An analysis of learning and recall..pdf:application/pdf},
}

@article{graham_holographic_nodate,
	title = {Holographic Generative Memory: Neurally Inspired One-Shot Learning with Memory Augmented Neural Networks},
	author = {Graham, Dillon R},
	langid = {english},
	file = {PDF:/home/connorh/Zotero/storage/RNSM8UUA/Graham - Holographic Generative Memory Neurally Inspired One-Shot Learning with Memory Augmented Neural Netw.pdf:application/pdf},
}

@inproceedings{gariaev_holographic_1991,
	location = {Zvenigorod, Russian Federation},
	title = {Holographic associative memory of biological systems},
	url = {http://proceedings.spiedigitallibrary.org/proceeding.aspx?articleid=982405},
	doi = {10.1117/12.50435},
	abstract = {We consider some specific problems and phenomena of morphogenetic infonnation storage, reproduction and transfer including phantom leaf effeot and field—induced morphogenetic translations between different taxo—nomic units. Several experimental results are presented and their expla—nation is given using a new approach to morphogenesis which combines some physical models or holographic associative memory and mathematical formalism of Fermi — Pasta — Ulam recurrence for solitary waves in deoxyribo—nucleic acid.},
	eventtitle = {Optical Memory and Neural Networks},
	pages = {280--291},
	author = {Gariaev, Peter P. and Chudin, Viktor I. and Komissarov, Gennady G. and Berezin, Andrey A. and Vasiliev, Anatoly A.},
	editor = {Mikaelian, Andrei L.},
	urldate = {2025-09-22},
	date = {1991-11-01},
	langid = {english},
	file = {PDF:/home/connorh/Zotero/storage/48GKN9R6/Gariaev et al. - 1991 - Holographic associative memory of biological systems.pdf:application/pdf},
}

@article{knight_holographic_1975,
	title = {Holographic Associative Memory and Processor},
	volume = {14},
	rights = {https://doi.org/10.1364/{OA}\_License\_v1\#{VOR}},
	issn = {0003-6935, 1539-4522},
	url = {https://opg.optica.org/abstract.cfm?URI=ao-14-5-1088},
	doi = {10.1364/AO.14.001088},
	pages = {1088},
	number = {5},
	journaltitle = {Applied Optics},
	shortjournal = {Appl. Opt.},
	author = {Knight, Gordon R.},
	urldate = {2025-09-22},
	date = {1975-05-01},
	langid = {english},
	file = {PDF:/home/connorh/Zotero/storage/VNBQMMYJ/Knight - 1975 - Holographic Associative Memory and Processor.pdf:application/pdf},
}

@article{psaltis_higher_1988,
	title = {Higher order associative memories and their optical implementations},
	volume = {1},
	rights = {https://www.elsevier.com/tdm/userlicense/1.0/},
	issn = {08936080},
	url = {https://linkinghub.elsevier.com/retrieve/pii/0893608088900172},
	doi = {10.1016/0893-6080(88)90017-2},
	abstract = {The properties of higher order memories are described. The non-redundant, up to Nth order polynomial expansion of N-dimensional binary vectors is shown to yield orthogonal feature vectors. The properties of expansions that contain only a single order are investigated in detail and the use of the sum of outer product algorithm for training higher order memories is analyzed. Optical implementations of quadratic associative memories"are described using volume holograms for the general case and planar holograms for shift invariant memories.},
	pages = {149--163},
	number = {2},
	journaltitle = {Neural Networks},
	shortjournal = {Neural Networks},
	author = {Psaltis, Demetri and Park, Cheol Hoon and Hong, John},
	urldate = {2025-09-22},
	date = {1988-01},
	langid = {english},
	file = {PDF:/home/connorh/Zotero/storage/G9UV54YF/Psaltis et al. - 1988 - Higher order associative memories and their optical implementations.pdf:application/pdf},
}

@article{gabor_associative_1969,
	title = {Associative Holographic Memories},
	volume = {13},
	issn = {0018-8646, 0018-8646},
	url = {http://ieeexplore.ieee.org/document/5391791/},
	doi = {10.1147/rd.132.0156},
	abstract = {Recently Longuet-Higgins modeled a temporal analogue of the property of holograms that allows a complete image to be constructed from only a portion of the hologram. In the present paper a more general analogue is discussed and two two-step transformations that imitate the recording-reconstruction sequence in holography are presented. The first transformation models the recall of an entire sequence from a fragment while the second is more like human memory in that it provides recall of only the part of the sequence that follows the keying fragment. Both models requireonlythe three operations: shift, multiplication and addition.},
	pages = {156--159},
	number = {2},
	journaltitle = {{IBM} Journal of Research and Development},
	shortjournal = {{IBM} J. Res. \& Dev.},
	author = {Gabor, D.},
	urldate = {2025-09-22},
	date = {1969-03},
	langid = {english},
	file = {PDF:/home/connorh/Zotero/storage/E2KGWMS5/Gabor - 1969 - Associative Holographic Memories.pdf:application/pdf},
}

@inproceedings{rutledge-taylor_holographic_2008,
	location = {London, United Kingdom},
	title = {A holographic associative memory recommender system},
	isbn = {978-1-4244-2916-5},
	url = {http://ieeexplore.ieee.org/document/4746700/},
	doi = {10.1109/ICDIM.2008.4746700},
	abstract = {We describe a recommender system based on Dynamically Structured Holographic Memory ({DSHM}), a cognitive model of associative memory that uses holographic reduced representations as the basis for its encoding of object associations. We compare this recommender to a conventional user-based collaborative ﬁltering algorithm on three datasets: {MovieLens}, and two bibliographic datasets such as those typically found in a digital library. Off-line experiments show that the holographic recommender is competitive in accuracy for predicting movie preferences and more accurate than collaborative ﬁltering on very sparse data sets. However, {DSHM} requires signiﬁcant amounts of computational resources which may may require a distributed implementation for it to be practical as a recommender for large data sets.},
	eventtitle = {2008 Third International Conference on Digital Information Management ({ICDIM})},
	pages = {87--92},
	booktitle = {2008 Third International Conference on Digital Information Management},
	publisher = {{IEEE}},
	author = {Rutledge-Taylor, Matthew F. and Vellino, Andre and West, Robert L.},
	urldate = {2025-09-22},
	date = {2008-11},
	langid = {english},
	file = {PDF:/home/connorh/Zotero/storage/S5RV4KEX/Rutledge-Taylor et al. - 2008 - A holographic associative memory recommender system.pdf:application/pdf},
}

@incollection{mcclelland_appeal_1986,
	title = {The Appeal of Parallel Distributed Processing},
	volume = {1},
	isbn = {978-0-262-29140-8},
	url = {https://doi.org/10.7551/mitpress/5236.001.0001},
	volumes = {2},
	pages = {3--44},
	booktitle = {Parallel Distributed Processing: Explorations in the Microstructure of Cognition: Foundations},
	publisher = {The {MIT} Press},
	author = {{McClelland}, James L. and Rumelhart, David E. and Hinton, Geoffrey E.},
	urldate = {2025-08-28},
	date = {1986-07-17},
}

@incollection{rumelhart_general_1986,
	title = {A General Framework for Parallel Distributed Processing},
	volume = {1},
	isbn = {978-0-262-29140-8},
	url = {https://doi.org/10.7551/mitpress/5236.001.0001},
	volumes = {2},
	pages = {45--76},
	booktitle = {Parallel Distributed Processing: Explorations in the Microstructure of Cognition: Foundations},
	publisher = {The {MIT} Press},
	author = {Rumelhart, David E. and Hinton, Geoffrey E. and {McClelland}, James L.},
	urldate = {2025-08-28},
	date = {1986-07-17},
}

@inproceedings{psaltis_nonlinear_1986,
	title = {Nonlinear discriminant functions and associative memories},
	volume = {151},
	url = {https://pubs.aip.org/aip/acp/article/151/1/370-375/755664},
	doi = {10.1063/1.36241},
	eventtitle = {{AIP} Conference Proceedings Volume 151},
	pages = {370--375},
	booktitle = {{AIP} Conference Proceedings},
	publisher = {{AIP}},
	author = {Psaltis, Demetri and {Cheol Hoon Park}},
	urldate = {2025-09-20},
	date = {1986},
	langid = {english},
	note = {{ISSN}: 0094243X},
	file = {PDF:/home/connorh/Zotero/storage/N3QVUP85/Psaltis and Cheol Hoon Park - 1986 - Nonlinear discriminant functions and associative memories.pdf:application/pdf},
}

@inproceedings{chen_high_1986,
	title = {High order correlation model for associative memory},
	volume = {151},
	url = {https://pubs.aip.org/aip/acp/article/151/1/86-99/755722},
	doi = {10.1063/1.36224},
	abstract = {A neural network model of associative memory with higher order learning rule is presented. The new model could be fashioned to either the auto-associative or the multiple associative mode. Energy function, asynchronous or synchronous dynamics can be constructed. The retrieval of the stored patterns or pattern sets from an incomplete input is monotonic guaranteed by a convergence theorem. The higher-order correlation model shows dramatic improvement in its storage capacity in comparison to the conventional binary correlation model. It also opens up the possibility of storing spatial-temporal patterns and symmetry invariant patterns.},
	eventtitle = {{AIP} Conference Proceedings Volume 151},
	pages = {86--99},
	booktitle = {{AIP} Conference Proceedings},
	publisher = {{AIP}},
	author = {Chen, H. H. and Lee, Y. C. and Sun, G. Z. and Lee, H. Y. and Maxwell, Tom and Giles, C. Lee},
	urldate = {2025-09-20},
	date = {1986},
	langid = {english},
	note = {{ISSN}: 0094243X},
	file = {PDF:/home/connorh/Zotero/storage/UM8WPFBA/Chen et al. - 1986 - High order correlation model for associative memory.pdf:application/pdf},
}

@misc{mimura_dynamical_2025,
	title = {Dynamical Properties of Dense Associative Memory},
	url = {http://arxiv.org/abs/2506.00851},
	doi = {10.48550/arXiv.2506.00851},
	abstract = {The dense associative memory is one of the basic modern Hopfield networks and can store large numbers of memory patterns. While the stationary state storage capacity has been investigated so far, its dynamical properties have not been discussed. In this paper, we analyze the dynamics by means of an exact approach based on generating functional analysis. It allows us to investigate convergence properties as well as the size of the attraction basins. We also analyze the stationary state of the updating rule.},
	number = {{arXiv}:2506.00851},
	publisher = {{arXiv}},
	author = {Mimura, Kazushi and Takeuchi, Jun'ichi and Sumikawa, Yuto and Kabashima, Yoshiyuki and Coolen, Anthony C. C.},
	urldate = {2025-09-20},
	date = {2025-06-01},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2506.00851 [cond-mat]},
	keywords = {Condensed Matter - Disordered Systems and Neural Networks},
	file = {PDF:/home/connorh/Zotero/storage/EVVKBEXG/Mimura et al. - 2025 - Dynamical Properties of Dense Associative Memory.pdf:application/pdf},
}

@book{haykin_neural_2009,
	location = {New York Munich},
	edition = {3. ed},
	title = {Neural networks and learning machines},
	isbn = {978-0-13-147139-9},
	pagetotal = {934},
	publisher = {Prentice-Hall},
	author = {Haykin, Simon S.},
	date = {2009},
	langid = {english},
	file = {PDF:/home/connorh/Zotero/storage/2BQ8Y6HK/Haykin - 2009 - Neural networks and learning machines.pdf:application/pdf},
}

@article{amit_storing_1985,
	title = {Storing Infinite Numbers of Patterns in a Spin-Glass Model of Neural Networks},
	volume = {55},
	rights = {http://link.aps.org/licenses/aps-default-license},
	issn = {0031-9007},
	url = {https://link.aps.org/doi/10.1103/PhysRevLett.55.1530},
	doi = {10.1103/PhysRevLett.55.1530},
	pages = {1530--1533},
	number = {14},
	journaltitle = {Physical Review Letters},
	shortjournal = {Phys. Rev. Lett.},
	author = {Amit, Daniel J. and Gutfreund, Hanoch and Sompolinsky, H.},
	urldate = {2025-09-17},
	date = {1985-09-30},
	langid = {english},
	file = {PDF:/home/connorh/Zotero/storage/2PSSJFIU/Amit et al. - 1985 - Storing Infinite Numbers of Patterns in a Spin-Glass Model of Neural Networks.pdf:application/pdf},
}

@article{amit_statistical_1987,
	title = {Statistical mechanics of neural networks near saturation},
	volume = {173},
	rights = {https://www.elsevier.com/tdm/userlicense/1.0/},
	issn = {00034916},
	url = {https://linkinghub.elsevier.com/retrieve/pii/0003491687900923},
	doi = {10.1016/0003-4916(87)90092-3},
	pages = {30--67},
	number = {1},
	journaltitle = {Annals of Physics},
	shortjournal = {Annals of Physics},
	author = {Amit, Daniel J and Gutfreund, Hanoch and Sompolinsky, H},
	urldate = {2025-09-17},
	date = {1987-01},
	langid = {english},
	file = {PDF:/home/connorh/Zotero/storage/3YX744FI/Amit et al. - 1987 - Statistical mechanics of neural networks near saturation.pdf:application/pdf},
}

@article{storkey_basins_1999,
	title = {The basins of attraction of a new Hopfield learning rule},
	volume = {12},
	rights = {https://www.elsevier.com/tdm/userlicense/1.0/},
	issn = {08936080},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0893608099000386},
	doi = {10.1016/S0893-6080(99)00038-6},
	abstract = {The nature of the basins of attraction of a Hopﬁeld network is as important as the capacity. Here a new learning rule is re-introduced. This learning rule has a higher capacity than that of the Hebb rule, and still keeps important functionality, such as incrementality and locality, which the pseudo-inverse lacks. However the basins of attraction of the ﬁxed points of this learning rule have not yet been studied. Three important characteristics of basins of attraction are considered: indirect and direct basins of attraction, distribution of sizes of basins of attraction and the shape of the basins of attraction. The results for the new learning rule are compared with those of the Hebb rule. The size of direct and indirect basins of attractions are generally larger for the new rule than for the Hebb rule, the distribution of sizes is more even, and the shape of the basins more round. q 1999 Elsevier Science Ltd. All rights reserved.},
	pages = {869--876},
	number = {6},
	journaltitle = {Neural Networks},
	shortjournal = {Neural Networks},
	author = {Storkey, A.J. and Valabregue, R.},
	urldate = {2025-09-17},
	date = {1999-07},
	langid = {english},
	file = {PDF:/home/connorh/Zotero/storage/R2D6MDY5/Storkey and Valabregue - 1999 - The basins of attraction of a new Hopfield learning rule.pdf:application/pdf},
}

@misc{mcalister_sequential_2025,
	title = {Sequential Learning in the Dense Associative Memory},
	url = {http://arxiv.org/abs/2409.15729},
	doi = {10.48550/arXiv.2409.15729},
	abstract = {Sequential learning involves learning tasks in a sequence, and proves challenging for most neural networks. Biological neural networks regularly conquer the sequential learning challenge and are even capable of transferring knowledge both forward and backwards between tasks. Artificial neural networks often totally fail to transfer performance between tasks, and regularly suffer from degraded performance or catastrophic forgetting on previous tasks. Models of associative memory have been used to investigate the discrepancy between biological and artificial neural networks due to their biological ties and inspirations, of which the Hopfield network is the most studied model. The Dense Associative Memory ({DAM}), or modern Hopfield network, generalizes the Hopfield network, allowing for greater capacities and prototype learning behaviors, while still retaining the associative memory structure. We give a substantial review of the sequential learning space with particular respect to the Hopfield network and associative memories. We perform foundational benchmarks of sequential learning in the {DAM} using various sequential learning techniques, and analyze the results of the sequential learning to demonstrate previously unseen transitions in the behavior of the {DAM}. This paper also discusses the departure from biological plausibility that may affect the utility of the {DAM} as a tool for studying biological neural networks. We present our findings, including the effectiveness of a range of state-of-the-art sequential learning methods when applied to the {DAM}, and use these methods to further the understanding of {DAM} properties and behaviors.},
	number = {{arXiv}:2409.15729},
	publisher = {{arXiv}},
	author = {{McAlister}, Hayden and Robins, Anthony and Szymanski, Lech},
	urldate = {2025-09-17},
	date = {2025-03-04},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2409.15729 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Neural and Evolutionary Computing},
	file = {PDF:/home/connorh/Zotero/storage/92DR35G6/McAlister et al. - 2025 - Sequential Learning in the Dense Associative Memory.pdf:application/pdf},
}

@article{sutton_learning_1988,
	title = {Learning to predict by the methods of temporal differences},
	volume = {3},
	rights = {http://www.springer.com/tdm},
	issn = {0885-6125, 1573-0565},
	url = {http://link.springer.com/10.1007/BF00115009},
	doi = {10.1007/BF00115009},
	abstract = {This article introduces a class of incremental learning procedures specialized for prediction that is, for using past experience with an incompletely known system to predict its future behavior. Whereas conventional prediction-learning methods assign credit by means of the difference between predicted and actual outcomes, tile new methods assign credit by means of the difference between temporally successive predictions. Although such temporal-difference method{\textasciitilde} have been used in Samuel's checker player, Holland's bucket brigade, and the author's Adaptive Heuristic Critic, they have remained poorly understood. Here we prove their convergence and optimality for special cases and relate them to supervised-learning methods. For most real-world prediction problems, telnporal-differenee methods require less memory and less peak computation than conventional methods and they produce more accurate predictions. We argue that most problems to which supervised learning is currently applied are really prediction problems of the sort to which temporaldifference methods can be applied to advantage.},
	pages = {9--44},
	number = {1},
	journaltitle = {Machine Learning},
	shortjournal = {Mach Learn},
	author = {Sutton, Richard S.},
	urldate = {2025-09-17},
	date = {1988-08},
	langid = {english},
	file = {PDF:/home/connorh/Zotero/storage/SGL6WB7S/Sutton - 1988 - Learning to predict by the methods of temporal differences.pdf:application/pdf},
}

@article{begin_categorization_1996,
	title = {Categorization in unsupervised neural networks: the Eidos model},
	volume = {7},
	rights = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/{IEEE}.html},
	issn = {1045-9227, 1941-0093},
	url = {https://ieeexplore.ieee.org/document/478399/},
	doi = {10.1109/72.478399},
	shorttitle = {Categorization in unsupervised neural networks},
	abstract = {Proulx and {BCgin} [5] recentlyexplained the power of a learningrule that {combinesHebbianand} anti-Hebbianlearning in unsupervisedauto-associativeneural networks.Combinedwith the brain-state-in-a-boxtransmission rule [2], this learning rule defines a new model of categorization: the Eidos model. To test this model, a simulated neural network, composed of 35 interconnected units, is subjected to an alphabetical characters recognition task. The results indicate the necessity of adding two parameters to the model: A restraining parameter and a forgetting parameter. The study shows the outstanding capacity of the model to categorize highly altered stimuli after a suitable learningprocess.Thus,the Eidos model seemsto be an interesting optionto achievecategorizationin unsupervisedneural networks.},
	pages = {147--154},
	number = {1},
	journaltitle = {{IEEE} Transactions on Neural Networks},
	shortjournal = {{IEEE} Trans. Neural Netw.},
	author = {Begin, J. and Proulx, R.},
	urldate = {2025-09-17},
	date = {1996-01},
	langid = {english},
	file = {PDF:/home/connorh/Zotero/storage/GG9UADLT/Begin and Proulx - 1996 - Categorization in unsupervised neural networks the Eidos model.pdf:application/pdf},
}

@article{alonso_sparse_2024,
	title = {A sparse quantized hopfield network for online-continual memory},
	volume = {15},
	issn = {2041-1723},
	url = {https://www.nature.com/articles/s41467-024-46976-4},
	doi = {10.1038/s41467-024-46976-4},
	abstract = {Abstract
            An important difference between brains and deep neural networks is the way they learn. Nervous systems learn online where a stream of noisy data points are presented in a non-independent, identically distributed way. Further, synaptic plasticity in the brain depends only on information local to synapses. Deep networks, on the other hand, typically use non-local learning algorithms and are trained in an offline, non-noisy, independent, identically distributed setting. Understanding how neural networks learn under the same constraints as the brain is an open problem for neuroscience and neuromorphic computing. A standard approach to this problem has yet to be established. In this paper, we propose that discrete graphical models that learn via an online maximum a posteriori learning algorithm could provide such an approach. We implement this kind of model in a neural network called the Sparse Quantized Hopfield Network. We show our model outperforms state-of-the-art neural networks on associative memory tasks, outperforms these networks in online, continual settings, learns efficiently with noisy inputs, and is better than baselines on an episodic memory task.},
	pages = {3722},
	number = {1},
	journaltitle = {Nature Communications},
	shortjournal = {Nat Commun},
	author = {Alonso, Nicholas and Krichmar, Jeffrey L.},
	urldate = {2025-09-17},
	date = {2024-05-02},
	langid = {english},
	file = {PDF:/home/connorh/Zotero/storage/J55EZDJM/Alonso and Krichmar - 2024 - A sparse quantized hopfield network for online-continual memory.pdf:application/pdf},
}

@article{mcalister_sequential_2025-1,
	title = {Sequential Learning in the Dense Associative Memory.},
	rights = {© 2025 Massachusetts Institute of Technology.},
	issn = {1530-888X 0899-7667},
	doi = {10.1162/neco.a.20},
	abstract = {Sequential learning involves learning tasks in a sequence and proves challenging for most neural networks. Biological neural networks regularly succeed at the  sequential learning challenge and are even capable of transferring knowledge both  forward and backward between tasks. Artificial neural networks often totally fail  to transfer performance between tasks and regularly suffer from degraded  performance or catastrophic forgetting on previous tasks. Models of associative  memory have been used to investigate the discrepancy between biological and  artificial neural networks due to their biological ties and inspirations, of  which the Hopfield network is the most studied model. The dense associative  memory ({DAM}), or modern Hopfield network, generalizes the Hopfield network,  allowing for greater capacities and prototype learning behaviors while still  retaining the associative memory structure. We give a substantial review of the  sequential learning space with particular respect to the Hopfield network and  associative memories. We present the first published benchmarks of sequential  learning in the {DAM} using various sequential learning techniques and analyze the  results of the sequential learning to demonstrate previously unseen transitions  in the behavior of the {DAM}. This letter also discusses the departure from  biological plausibility that may affect the utility of the {DAM} as a tool for  studying biological neural networks. We present our findings, including the  effectiveness of a range of state-of-the-art sequential learning methods when  applied to the {DAM}, and use these methods to further the understanding of {DAM}  properties and behaviors.},
	pages = {1--48},
	journaltitle = {Neural computation},
	shortjournal = {Neural Comput},
	author = {{McAlister}, Hayden and Robins, Anthony and Szymanski, Lech},
	date = {2025-07-24},
	pmid = {40705090},
	note = {Place: United States},
}

@incollection{kanerva_sparse_1993,
	location = {New York},
	title = {Sparse Distributed Memory and Related Models},
	pages = {50--76},
	booktitle = {Associative Neural Memories: Theory and Implementation},
	publisher = {Oxford University Press},
	author = {Kanerva, Pentti},
	date = {1993},
	langid = {english},
	file = {PDF:/home/connorh/Zotero/storage/Q564FQ5Z/Kanerva - Sparse Distributed Memory and Related Models.pdf:application/pdf},
}

@misc{hu_provably_2024,
	title = {Provably Optimal Memory Capacity for Modern Hopfield Models: Transformer-Compatible Dense Associative Memories as Spherical Codes},
	url = {http://arxiv.org/abs/2410.23126},
	doi = {10.48550/arXiv.2410.23126},
	shorttitle = {Provably Optimal Memory Capacity for Modern Hopfield Models},
	abstract = {We study the optimal memorization capacity of modern Hopfield models and Kernelized Hopfield Models ({KHMs}), a transformer-compatible class of Dense Associative Memories. We present a tight analysis by establishing a connection between the memory configuration of {KHMs} and spherical codes from information theory. Specifically, we treat the stored memory set as a specialized spherical code. This enables us to cast the memorization problem in {KHMs} into a point arrangement problem on a hypersphere. We show that the optimal capacity of {KHMs} occurs when the feature space allows memories to form an optimal spherical code. This unique perspective leads to: (i) An analysis of how {KHMs} achieve optimal memory capacity, and identify corresponding necessary conditions. Importantly, we establish an upper capacity bound that matches the well-known exponential lower bound in the literature. This provides the first tight and optimal asymptotic memory capacity for modern Hopfield models. (ii) A sub-linear time algorithm U-Hop+ to reach {KHMs}’ optimal capacity. (iii) An analysis of the scaling behavior of the required feature dimension relative to the number of stored memories. These efforts improve both the retrieval capability of {KHMs} and the representation learning of corresponding transformers. Experimentally, we provide thorough numerical results to back up theoretical findings.},
	number = {{arXiv}:2410.23126},
	publisher = {{arXiv}},
	author = {Hu, Jerry Yao-Chieh and Wu, Dennis and Liu, Han},
	urldate = {2025-09-16},
	date = {2024-10-31},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2410.23126 [stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Neural and Evolutionary Computing, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {PDF:/home/connorh/Zotero/storage/NL23J6K9/Hu et al. - 2024 - Provably Optimal Memory Capacity for Modern Hopfield Models Transformer-Compatible Dense Associativ.pdf:application/pdf},
}

@article{little_existence_1974,
	title = {The existence of persistent states in the brain},
	volume = {19},
	issn = {0025-5564},
	doi = {https://doi.org/10.1016/0025-5564(74)90031-5.},
	pages = {101--120},
	number = {1},
	journaltitle = {Mathematical Biosciences},
	author = {Little, {WA}},
	date = {1974-02},
	file = {the_existence_of_persistent_states_in_the_brain:/home/connorh/Zotero/storage/IHHIZNBJ/the_existence_of_persistent_states_in_the_brain.pdf:application/pdf},
}

@article{marr_simple_1971,
	title = {Simple Memory: A Theory for Archicortex},
	volume = {262},
	series = {B},
	pages = {23--81},
	number = {841},
	journaltitle = {Philosophical Transactions of the Royal Society of London},
	author = {Marr, D},
	date = {1971-07-01},
	langid = {english},
	file = {PDF:/home/connorh/Zotero/storage/FP5IHJGQ/Marr - Simple Memory A Theory for Archicortex.pdf:application/pdf},
}

@article{stanley_simulation_1976,
	title = {Simulation studies of a temporal sequence memory model},
	volume = {24},
	rights = {http://www.springer.com/tdm},
	issn = {0340-1200, 1432-0770},
	url = {http://link.springer.com/10.1007/BF00364115},
	doi = {10.1007/BF00364115},
	abstract = {Models of circuit action in the mammalian hippocampus have led us to a study of habituation circuits. In order to help model the process of habituation we consider here a memory network designed to learn sequences of inputs separated by various time intervals and to repeat these sequences when cued by their initial portions. The structure of the memory is based on the anatomy of the dentate gyrus region of the mammalian hippocampus. The model consists of a number of arrays of cells called lamellae. Each array consists of four lines of model cells coupled uniformly to neighbors within the array and with some randomness to cells in other lamellae. All model cells operate according to first-order differential equations. Two of the lines of cells in each lamella are coupled such that sufficient excitation by a system input generates a wave of activity that travels down the lamella. Such waves effect dynamic storage of the representation of each input, allowing association connections to form that code both the set of cells stimulated by each input and the time interval between successive inputs. Results of simulation of two networks are presented illustrating the model's operating characteristics and memory capacity.},
	pages = {121--137},
	number = {3},
	journaltitle = {Biological Cybernetics},
	shortjournal = {Biol. Cybernetics},
	author = {Stanley, J. C.},
	urldate = {2025-09-16},
	date = {1976},
	langid = {english},
	file = {PDF:/home/connorh/Zotero/storage/MJGZ69BP/Stanley - 1976 - Simulation studies of a temporal sequence memory model.pdf:application/pdf},
}

@article{amari_learning_1972,
	title = {Learning Patterns and Pattern Sequences by Self-Organizing Nets of Threshold Elements},
	volume = {C-21},
	rights = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/{IEEE}.html},
	issn = {0018-9340, 1557-9956, 2326-3814},
	url = {https://ieeexplore.ieee.org/document/1672070/},
	doi = {10.1109/T-C.1972.223477},
	abstract = {Various information-processing capabilities of selforganizing nets of threshold elements are studied. A self-organizing net, learning from patterns or pattern sequences given from outside as stimuli, "remembers" some of them as stable equilibrium states or state-transition sequences of the net. A condition where many patterns and pattern sequences are remembered in a net at the same time is shown. The stability degree of their remembrance and recalling under noise disturbances is investigated theoretically. For this purpose, the stability of state transition in an autonomous logical net of threshold elements is studied by the use of characteristics of threshold elements.},
	pages = {1197--1206},
	number = {11},
	journaltitle = {{IEEE} Transactions on Computers},
	shortjournal = {{IEEE} Trans. Comput.},
	author = {Amari, S.-I.},
	urldate = {2025-09-16},
	date = {1972-11},
	langid = {english},
	file = {PDF:/home/connorh/Zotero/storage/Y3Q7L74N/Amari - 1972 - Learning Patterns and Pattern Sequences by Self-Organizing Nets of Threshold Elements.pdf:application/pdf},
}

@article{nakano_associatron-model_1972,
	title = {Associatron-A Model of Associative Memory},
	volume = {{SMC}-2},
	rights = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/{IEEE}.html},
	issn = {0018-9472, 2168-2909},
	url = {http://ieeexplore.ieee.org/document/4309133/},
	doi = {10.1109/TSMC.1972.4309133},
	abstract = {Thinking in the human brain greatly depends upon association mechanisms which can be utilized in machine intelligence. An associative memory device, called "Associatron," is proposed. The Associatron stores entities represented by bit patterns in a distributed manner and recalls the whole of any entity from a part of it. If the part is large, the recalled entity will be accurate; on the other hand, if the part is small, the recalled entity will be rather ambiguous. Any number of entities can be stored, but the accuracy of the recalled entity decreases as the number of entities increases. The Associatron is considered to be a simplified model of the neural network and can be constructed as a cellular structure, where each cell is connected to only its neighbor cells and all cells run in parallel. From its mechanisms some properties are derived that are expected to be utilized for human-like information processing. After these properties have been analyzed, an Associatron which deals with entities composed of less than 180 bits is simulated by a computer. Simple examples of its applications for concept formation and game playing are presented and the thinking process by the sequence of associations is described.},
	pages = {380--388},
	number = {3},
	journaltitle = {{IEEE} Transactions on Systems, Man, and Cybernetics},
	shortjournal = {{IEEE} Trans. Syst., Man, Cybern.},
	author = {Nakano, Kaoru},
	urldate = {2025-09-16},
	date = {1972-07},
	langid = {english},
	file = {PDF:/home/connorh/Zotero/storage/VMFZXXF7/Nakano - 1972 - Associatron-A Model of Associative Memory.pdf:application/pdf},
}

@misc{millidge_universal_2022,
	title = {Universal Hopfield Networks: A General Framework for Single-Shot Associative Memory Models},
	url = {http://arxiv.org/abs/2202.04557},
	doi = {10.48550/arXiv.2202.04557},
	shorttitle = {Universal Hopfield Networks},
	abstract = {A large number of neural network models of associative memory have been proposed in the literature. These include the classical Hopﬁeld networks ({HNs}), sparse distributed memories ({SDMs}), and more recently the modern continuous Hopﬁeld networks ({MCHNs}), which possess close links with self-attention in machine learning. In this paper, we propose a general framework for understanding the operation of such memory networks as a sequence of three operations: similarity, separation, and projection. We derive all these memory models as instances of our general framework with differing similarity and separation functions. We extend the mathematical framework of Krotov and Hopﬁeld (2020) to express general associative memory models using neural network dynamics with local computation, and derive a general energy function that is a Lyapunov function of the dynamics. Finally, using our framework, we empirically investigate the capacity of using different similarity functions for these associative memory models, beyond the dot product similarity measure, and demonstrate empirically that Euclidean or Manhattan distance similarity metrics perform substantially better in practice on many tasks, enabling a more robust retrieval and higher memory capacity than existing models.},
	number = {{arXiv}:2202.04557},
	publisher = {{arXiv}},
	author = {Millidge, Beren and Salvatori, Tommaso and Song, Yuhang and Lukasiewicz, Thomas and Bogacz, Rafal},
	urldate = {2025-08-17},
	date = {2022-06-17},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2202.04557 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Neural and Evolutionary Computing, Computer Science - Machine Learning},
	file = {PDF:/home/connorh/Zotero/storage/QBPB76SP/Millidge et al. - 2022 - Universal Hopfield Networks A General Framework for Single-Shot Associative Memory Models.pdf:application/pdf;PDF:/home/connorh/Zotero/storage/IG6FUK6L/Millidge et al. - 2022 - Universal Hopfield Networks A General Framework for Single-Shot Associative Memory Models.pdf:application/pdf},
}

@article{geigerman_bind_2016,
	title = {To bind or not to bind, that's the wrong question: Features and objects coexist in visual short-term memory},
	volume = {167},
	issn = {00016918},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0001691816300701},
	doi = {10.1016/j.actpsy.2016.04.004},
	shorttitle = {To bind or not to bind, that's the wrong question},
	pages = {45--51},
	journaltitle = {Acta Psychologica},
	shortjournal = {Acta Psychologica},
	author = {Geigerman, Shriradha and Verhaeghen, Paul and Cerella, John},
	urldate = {2025-08-27},
	date = {2016-06},
	langid = {english},
	file = {PDF:/home/connorh/Zotero/storage/VFUDL9F9/Geigerman et al. - 2016 - To bind or not to bind, that's the wrong question Features and objects coexist in visual short-term.pdf:application/pdf},
}

@article{brown_simplest_2008,
	title = {The simplest complete model of choice response time: Linear ballistic accumulation},
	volume = {57},
	rights = {https://www.elsevier.com/tdm/userlicense/1.0/},
	issn = {00100285},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0010028507000722},
	doi = {10.1016/j.cogpsych.2007.12.002},
	shorttitle = {The simplest complete model of choice response time},
	abstract = {We propose a linear ballistic accumulator ({LBA}) model of decision making and reaction time. The {LBA} is simpler than other models of choice response time, with independent accumulators that race towards a common response threshold. Activity in the accumulators increases in a linear and deterministic manner. The simplicity of the model allows complete analytic solutions for choices between any number of alternatives. These solutions (and freely-available computer code) make the model easy to apply to both binary and multiple choice situations. Using data from ﬁve previously published experiments, we demonstrate that the {LBA} model successfully accommodates empirical phenomena from binary and multiple choice tasks that have proven diﬃcult for other theoretical accounts. Our results are encouraging in a ﬁeld beset by the tradeoﬀ between complexity and completeness.},
	pages = {153--178},
	number = {3},
	journaltitle = {Cognitive Psychology},
	shortjournal = {Cognitive Psychology},
	author = {Brown, Scott D. and Heathcote, Andrew},
	urldate = {2025-08-27},
	date = {2008-11},
	langid = {english},
	file = {PDF:/home/connorh/Zotero/storage/LGVR8MMY/Brown and Heathcote - 2008 - The simplest complete model of choice response time Linear ballistic accumulation.pdf:application/pdf},
}

@article{osth_novelty_2023,
	title = {Novelty Rejection in Episodic Memory},
	volume = {130},
	abstract = {Episodic memory theories have postulated that in recognition, a probe is accepted or rejected on the basis of its global similarity to studied items. Mewhort and Johns (2000) directly tested global similarity predictions by manipulating the feature compositions of probes—novelty rejection was facilitated when probes contained novel features even when other features strongly matched, an advantage dubbed the extralist feature effect, which greatly challenged global matching models. In this work, we conducted similar experiments using continuously valued separable- and integral-dimension stimuli. Analogs of extralist lures were constructed where one stimulus dimension contained a value that was more novel than the other dimensions, whereas overall similarity was equated to another class of lures. Facilitated novelty rejection for lures with extralist features was only found for separable-dimension stimuli. While integral-dimension stimuli were well described by a global matching model, the model failed to account for extralist feature effects with separable-dimension stimuli. We applied global matching models—including variants of the exemplar-based linear ballistic accumulator—that employed different means of novelty rejection afforded by separable-dimension stimuli, including decisions based on the global similarity of the individual dimensions and selective attention being directed toward novel probe values (a diagnostic attention model). While these variants produced the extralist feature effect, only the diagnostic attention model succeeded in providing a sufﬁcient account of all of the data. The model was also able to account for extralist feature effects in an experiment with discrete features similar to those from Mewhort and Johns (2000).},
	pages = {720--769},
	number = {3},
	journaltitle = {Psychological Review},
	author = {Osth, Adam F and Zhou, Aspen and Lilburn, Simon D and Little, Daniel R},
	date = {2023-03-13},
	langid = {english},
	file = {PDF:/home/connorh/Zotero/storage/GQDSH5T4/Osth et al. - Novelty Rejection in Episodic Memory.pdf:application/pdf;PDF:/home/connorh/Zotero/storage/WY43HCLX/Osth et al. - Novelty Rejection in Episodic Memory.pdf:application/pdf},
}

@article{hopfield_neurons_1984,
	title = {Neurons with graded response have collective computational properties like those of two-state neurons.},
	volume = {81},
	issn = {0027-8424, 1091-6490},
	url = {https://pnas.org/doi/full/10.1073/pnas.81.10.3088},
	doi = {10.1073/pnas.81.10.3088},
	abstract = {A model for a large network of "neurons" with a graded response (or sigmoid input-output relation) is studied. This deterministic system has collective properties in very close correspondence with the earlier stochastic model based on {McCulloch}-Pitts neurons. The content-addressable memory and other emergent collective properties of the original model also are present in the graded response model. The idea that such collective properties are used in biological systems is given added credence by the continued presence of such properties for more nearly biological "neurons." Collective analog electrical circuits of the kind described will certainly function. The collective states of the two models have a simple correspondence. The original model will continue to be useful for simulations, because its connection to graded response systems is established. Equations that include the effect of action potentials in the graded response system are also developed.},
	pages = {3088--3092},
	number = {10},
	journaltitle = {Proceedings of the National Academy of Sciences},
	shortjournal = {Proc. Natl. Acad. Sci. U.S.A.},
	author = {Hopfield, J J},
	urldate = {2025-08-17},
	date = {1984-05},
	langid = {english},
	file = {PDF:/home/connorh/Zotero/storage/UYG9FMW8/Hopfield - 1984 - Neurons with graded response have collective computational properties like those of two-state neuron.pdf:application/pdf;PDF:/home/connorh/Zotero/storage/QURL2A3E/Hopfield - 1984 - Neurons with graded response have collective computational properties like those of two-state neuron.pdf:application/pdf},
}

@article{hopfield_neural_1982,
	title = {Neural networks and physical systems with emergent collective computational abilities.},
	volume = {79},
	abstract = {Computational properties of use to biological organisms or to the construction of computers can emerge as collective properties of systems -having a large number of simple equivalent components (or neurons). The physical meaning ofcontent-addressable memory is described by an appropriate phase space flow of the state of a system. A model of such a system is given, based on aspects of neurobiology but readily adapted to integrated circuits. The collective properties of this model produce a content-addressable memory which correctly yields an entire memory from any subpart of sufficient size. The algorithm for the time evolution of the state of the system is based on asynchronous parallel processing. Additional emergent collective properties include some capacity for generalization, familiarity recognition, categorization, error correction, and time sequence retention. The collective properties are only weakly sensitive to details ofthe modeling or the failure of individual devices.},
	pages = {2554--2558},
	journaltitle = {Proceedings of the National Academy of Sciences},
	author = {Hopfield, John},
	date = {1982-04},
	langid = {english},
	file = {PDF:/home/connorh/Zotero/storage/TV3XD4QV/Neural networks and physical systems with emergent collective computational abilities..pdf:application/pdf},
}

@misc{krotov_modern_2025,
	title = {Modern Methods in Associative Memory},
	url = {http://arxiv.org/abs/2507.06211},
	doi = {10.48550/arXiv.2507.06211},
	abstract = {Associative Memories like the famous Hopfield Networks are elegant models for describing fully recurrent neural networks whose fundamental job is to store and retrieve information. In the past few years they experienced a surge of interest due to novel theoretical results pertaining to their information storage capabilities, and their relationship with {SOTA} {AI} architectures, such as Transformers and Diffusion Models. These connections open up possibilities for interpreting the computation of traditional {AI} networks through the theoretical lens of Associative Memories. Additionally, novel Lagrangian formulations of these networks make it possible to design powerful distributed models that learn useful representations and inform the design of novel architectures. This tutorial provides an approachable introduction to Associative Memories, emphasizing the modern language and methods used in this area of research, with practical hands-on mathematical derivations and coding notebooks.},
	number = {{arXiv}:2507.06211},
	publisher = {{arXiv}},
	author = {Krotov, Dmitry and Hoover, Benjamin and Ram, Parikshit and Pham, Bao},
	urldate = {2025-08-17},
	date = {2025-07-08},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2507.06211 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {PDF:/home/connorh/Zotero/storage/FHAK5767/Krotov et al. - 2025 - Modern Methods in Associative Memory.pdf:application/pdf;PDF:/home/connorh/Zotero/storage/7JYQ3GJN/Krotov et al. - 2025 - Modern Methods in Associative Memory.pdf:application/pdf},
}

@misc{krotov_hierarchical_2021,
	title = {Hierarchical Associative Memory},
	url = {http://arxiv.org/abs/2107.06446},
	doi = {10.48550/arXiv.2107.06446},
	abstract = {Dense Associative Memories or Modern Hopﬁeld Networks have many appealing properties of associative memory. They can do pattern completion, store a large number of memories, and can be described using a recurrent neural network with a degree of biological plausibility and rich feedback between the neurons. At the same time, up until now all the models of this class have had only one hidden layer, and have only been formulated with densely connected network architectures, two aspects that hinder their machine learning applications. This paper tackles this gap and describes a fully recurrent model of associative memory with an arbitrary large number of layers, some of which can be locally connected (convolutional), and a corresponding energy function that decreases on the dynamical trajectory of the neurons’ activations. The memories of the full network are dynamically “assembled” using primitives encoded in the synaptic weights of the lower layers, with the “assembling rules” encoded in the synaptic weights of the higher layers. In addition to the bottom-up propagation of information, typical of commonly used feedforward neural networks, the model described has rich top-down feedback from higher layers that help the lower-layer neurons to decide on their response to the input stimuli.},
	number = {{arXiv}:2107.06446},
	publisher = {{arXiv}},
	author = {Krotov, Dmitry},
	urldate = {2025-08-18},
	date = {2021-07-14},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2107.06446 [cs]},
	keywords = {Computer Science - Neural and Evolutionary Computing, Quantitative Biology - Neurons and Cognition, Computer Science - Machine Learning, Statistics - Machine Learning, Condensed Matter - Disordered Systems and Neural Networks},
	file = {PDF:/home/connorh/Zotero/storage/F6VVTFP2/Krotov - 2021 - Hierarchical Associative Memory.pdf:application/pdf;PDF:/home/connorh/Zotero/storage/Y6FZZ37D/Krotov - 2021 - Hierarchical Associative Memory.pdf:application/pdf;PDF:/home/connorh/Zotero/storage/9A2T635M/Krotov - 2021 - Hierarchical Associative Memory.pdf:application/pdf},
}

@article{meyer-grant_extreme-value_nodate,
	title = {Extreme-Value Signal Detection Theory for Recognition Memory: The Parametric Road Not Taken},
	abstract = {Signal Detection Theory has long served as a cornerstone of psychological research, particularly in recognition memory. Yet its conventional application hinges almost exclusively on the Gaussian assumption—an adherence rooted more in historical convenience than theoretical necessity that comes with a number of well-documented drawbacks. In this work, we critically examine these limitations and introduce a principled parametric alternative: the Gumbelmin model, based on extreme-value distributions of event minima. A key feature of this model is its grounding in a behavioral principle of invariance under uniform choice-set expansions—a prediction we empirically validate in a novel recognition-memory experiment. We further benchmark the Gumbelmin model against its Gaussian counterpart across multiple recognition-memory tasks, including confidence-rating, ranking, forced-choice, and detection-plusidentification paradigms. Our findings highlight the model’s parsimonious yet successful characterization of recognition-memory judgments, as well as the utility of its associated discriminability index, g′, which can be directly computed from a single pair of hit and false-alarm rates.},
	author = {Meyer-Grant, Constantin G and Kellen, David and Singmann, Henrik},
	langid = {english},
	file = {PDF:/home/connorh/Zotero/storage/XVHE9N3C/Meyer-Grant et al. - Extreme-Value Signal Detection Theory for Recognition Memory The Parametric Road Not Taken.pdf:application/pdf},
}

@article{roger_ratcliff_theory_1978,
	title = {A Theory of Memory Retrieval.},
	volume = {85},
	doi = {10.1037/0033-295x.85.2.59},
	abstract = {A theory of memory retrieval is developed and is shown to apply over a range of experimental paradigms. Access to memory traces is viewed in terms of a resonance metaphor. The probe item evokes the search set on the basis of probe-memory item relatedness, just as a ringing tuning fork evokes sympathetic vibrations in other tuning forks. Evidence is accumulated in parallel from each probe-memory item comparison, and each comparison is modeled by a continuous random walk process. In item recognition, the decision process is self-terminating on matching comparisons and exhaustive on nonmatching comparisons. The mathematical model produces predictions about accuracy, mean reaction time, error latency, and reaction time distributions that are in good accord with experimental data. The theory is applied to four item recognition paradigms (Sternberg, prememorized list, study-test, and continuous) and to speed-accuracy paradigms; results are found to provide a basis for comparison of these paradigms. It is noted that neural network models can be interfaced to the retrieval theory with little difficulty and that semantic memory models may benefit from such a retrieval scheme.},
	pages = {59--108},
	number = {2},
	journaltitle = {Psychological Review},
	author = {{Roger Ratcliff} and Ratcliff, Roger},
	date = {1978-01-01},
	doi = {10.1037/0033-295x.85.2.59},
	note = {{MAG} {ID}: 2098205603},
	file = {PDF:/home/connorh/Zotero/storage/4JBT6I7Y/Ratcliff - A Theory of Memory Retrieval.pdf:application/pdf},
}

@article{johns_effect_2003,
	title = {The effect of feature frequency on short-term recognition memory},
	volume = {31},
	rights = {http://www.springer.com/tdm},
	issn = {0090-502X, 1532-5946},
	url = {http://link.springer.com/10.3758/BF03194387},
	doi = {10.3758/BF03194387},
	pages = {285--296},
	number = {2},
	journaltitle = {Memory \& Cognition},
	shortjournal = {Memory \& Cognition},
	author = {Johns, E. E. and Mewhort, D. J. K.},
	urldate = {2025-09-16},
	date = {2003-03},
	langid = {english},
	file = {PDF:/home/connorh/Zotero/storage/6VCG85E7/Johns and Mewhort - 2003 - The effect of feature frequency on short-term recognition memory.pdf:application/pdf},
}

@article{mewhort_sharpening_2005,
	title = {Sharpening the echo: An iterative‐resonance model for short‐term recognition memory},
	volume = {13},
	issn = {0965-8211, 1464-0686},
	url = {http://www.tandfonline.com/doi/full/10.1080/09658210344000242},
	doi = {10.1080/09658210344000242},
	shorttitle = {Sharpening the echo},
	pages = {300--307},
	number = {3},
	journaltitle = {Memory},
	shortjournal = {Memory},
	author = {Mewhort, D. J. K. and Johns, E. E.},
	urldate = {2025-09-16},
	date = {2005-03},
	langid = {english},
	file = {PDF:/home/connorh/Zotero/storage/CMCMMGW5/Mewhort and Johns - 2005 - Sharpening the echo An iterative‐resonance model for short‐term recognition memory.pdf:application/pdf},
}

@article{osth_integrating_2024,
	title = {Integrating word-form representations with global similarity computation in recognition memory},
	volume = {31},
	issn = {1069-9384, 1531-5320},
	url = {https://link.springer.com/10.3758/s13423-023-02402-2},
	doi = {10.3758/s13423-023-02402-2},
	abstract = {In recognition memory, retrieval is thought to occur by computing the global similarity of the probe to each of the studied items. However, to date, very few global similarity models have employed perceptual representations of words despite the fact that false recognition errors for perceptually similar words have consistently been observed. In this work, we integrate representations of letter strings from the reading literature with global similarity models. Speciﬁcally, we employed models of absolute letter position (slot codes and overlap models) and relative letter position (closed and open bigrams). Each of the representations was used to construct a global similarity model that made contact with responses and {RTs} at the individual word level using the linear ballistic accumulator ({LBA}) model (Brown \& Heathcote Cognitive Psychology, 57 , 153–178, 2008). Relative position models were favored in three of the four datasets and parameter estimates suggested additional inﬂuence of the initial letters in the words. When semantic representations from the word2vec model were incorporated into the models, results indicated that orthographic representations were almost equally consequential as semantic representations in determining inter-item similarity and false recognition errors, which undermines previous suggestions that long-term memory is primarily driven by semantic representations. The model was able to modestly capture individual word variability in the false alarm rates, but there were limitations in capturing variability in the hit rates that suggest that the underlying representations require extension.},
	pages = {1000--1031},
	number = {3},
	journaltitle = {Psychonomic Bulletin \& Review},
	shortjournal = {Psychon Bull Rev},
	author = {Osth, Adam F. and Zhang, Lyulei},
	urldate = {2025-09-16},
	date = {2024-06},
	langid = {english},
	file = {PDF:/home/connorh/Zotero/storage/UC3YAQDE/Osth and Zhang - 2024 - Integrating word-form representations with global similarity computation in recognition memory.pdf:application/pdf},
}

@article{pineda_imagery_2023,
	title = {Imagery in the entropic associative memory},
	volume = {13},
	issn = {2045-2322},
	url = {https://www.nature.com/articles/s41598-023-36761-6},
	doi = {10.1038/s41598-023-36761-6},
	abstract = {Abstract
            The Entropic Associative Memory is a novel declarative and distributed computational model of associative memory. The model is general, conceptually simple, and offers an alternative to models developed within the artificial neural networks paradigm. The memory uses a standard table as its medium, where the information is stored in an indeterminate form, and the entropy plays a functional and operation role. The memory register operation abstracts the input cue with the current memory content and is productive; memory recognition is performed through a logical test; and memory retrieval is constructive. The three operations can be performed in parallel using very few computing resources. In our previous work we explored the auto-associative properties of the memory and performed experiments to store, recognize and retrieve manuscript digits and letters with complete and incomplete cues, and also to recognize and learn phones, with satisfactory results. In such experiments a designated memory register was used to store all the objects of the same class, whereas in the present study we remove such restriction and use a single memory register to store all the objects in the domain. In this novel setting we explore the production of emerging objects and relations, such that cues are used not only to retrieve remembered objects, but also related and imaged objects, and to produce association chains. The present model supports the view that memory and classification are independent functions both conceptually and architecturally. The memory system can store images of the different modalities of perception and action, possibly multimodal, and offers a novel perspective on the imagery debate and computational models of declarative memory.},
	pages = {9553},
	number = {1},
	journaltitle = {Scientific Reports},
	shortjournal = {Sci Rep},
	author = {Pineda, Luis A. and Morales, Rafael},
	urldate = {2025-09-16},
	date = {2023-06-12},
	langid = {english},
	file = {PDF:/home/connorh/Zotero/storage/R3346T8P/Pineda and Morales - 2023 - Imagery in the entropic associative memory.pdf:application/pdf},
}

@article{malmberg_feature_2002,
	title = {Feature frequency effects in recognition memory},
	volume = {30},
	rights = {http://www.springer.com/tdm},
	issn = {0090-502X, 1532-5946},
	url = {http://link.springer.com/10.3758/BF03194962},
	doi = {10.3758/BF03194962},
	pages = {607--613},
	number = {4},
	journaltitle = {Memory \& Cognition},
	shortjournal = {Memory \& Cognition},
	author = {Malmberg, Kenneth J. and Steyvers, Mark and Stephens, Joseph D. and Shiffrin, Richard M.},
	urldate = {2025-09-16},
	date = {2002-06},
	langid = {english},
	file = {PDF:/home/connorh/Zotero/storage/DZAFSAZV/Malmberg et al. - 2002 - Feature frequency effects in recognition memory.pdf:application/pdf},
}

@article{osth_diffusion_2017,
	title = {Diffusion vs. linear ballistic accumulation: Different models, different conclusions about the slope of the {zROC} in recognition memory},
	volume = {96},
	issn = {0749596X},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0749596X16302042},
	doi = {10.1016/j.jml.2017.04.003},
	shorttitle = {Diffusion vs. linear ballistic accumulation},
	abstract = {The relative amount of variability in memory strength for targets vs. lures in recognition memory is commonly measured using the receiver operating characteristic ({ROC}) procedure, in which participants are given either a bias manipulation or are instructed to give conﬁdence ratings to probe items. A near universal ﬁnding is that targets have higher variability than lures. Ratcliff and Starns (2009) questioned the conclusions of the {ROC} procedure by demonstrating that accounting for decision noise within a response time model yields different conclusions about relative memory evidence than the {ROC} procedure yields. In an attempt to better understand the source of the discrepancy, we applied models that include different sources of decision noise, including both the diffusion decision model ({DDM}) and the linear ballistic accumulator ({LBA}) model, which either include or lack within-trial noise in evidence accumulation, and compared their estimates of the ratio of standard deviations to those from {ROC} analysis. Each method produced dramatically different estimates of the relative variability of target items, with the {LBA} even indicating equal variance in some cases. This stands in contrast to prior work suggesting that the {DDM} and {LBA} produce largely similar estimates of relevant model parameters, such as drift rate, boundary separation, and nondecision time. Parameter validation using data from Starns’s (2014) numerosity discrimination data demonstrated that only the {DDM} was able to correctly reproduce the evidence ratios in the data. These results suggest that the {DDM} may be providing a more accurate account of lure-to-target variability evidence ratios in recognition memory.},
	pages = {36--61},
	journaltitle = {Journal of Memory and Language},
	shortjournal = {Journal of Memory and Language},
	author = {Osth, Adam F. and Bora, Beatrice and Dennis, Simon and Heathcote, Andrew},
	urldate = {2025-09-16},
	date = {2017-10},
	langid = {english},
	file = {PDF:/home/connorh/Zotero/storage/PYF67QF4/Osth et al. - 2017 - Diffusion vs. linear ballistic accumulation Different models, different conclusions about the slope.pdf:application/pdf},
}

@article{johns_synchronization_2012,
	title = {A synchronization account of false recognition},
	volume = {65},
	rights = {https://www.elsevier.com/tdm/userlicense/1.0/},
	issn = {00100285},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0010028512000564},
	doi = {10.1016/j.cogpsych.2012.07.002},
	abstract = {We describe a computational model to explain a variety of results in both standard and false recognition. A key attribute of the model is that it uses plausible semantic representations for words, built through exposure to a linguistic corpus. A study list is encoded in the model as a gist trace, similar to the proposal of fuzzy trace theory (Brainerd \& Reyna, 2002), but based on realistically structured semantic representations of the component words. The model uses a decision process based on the principles of neural synchronization and information accumulation. The decision process operates by synchronizing a probe with the gist trace of a study context, allowing information to be accumulated about whether the word did or did not occur on the study list, and the efﬁciency of synchronization determines recognition. We demonstrate that the model is capable of accounting for standard recognition results that are challenging for classic global memory models, and can also explain a wide variety of false recognition effects and make item-speciﬁc predictions for critical lures. The model demonstrates that both standard and false recognition results may be explained within a single formal framework by integrating realistic representation assumptions with a simple processing mechanism.},
	pages = {486--518},
	number = {4},
	journaltitle = {Cognitive Psychology},
	shortjournal = {Cognitive Psychology},
	author = {Johns, Brendan T. and Jones, Michael N. and Mewhort, Douglas J.K.},
	urldate = {2025-09-16},
	date = {2012-12},
	langid = {english},
	file = {PDF:/home/connorh/Zotero/storage/UNKQT5U7/Johns et al. - 2012 - A synchronization account of false recognition.pdf:application/pdf},
}

@article{johns_continuous_nodate,
	title = {A Continuous Source Reinstatement Model of True and Illusory Recollection},
	abstract = {In studies of false recognition, subjects not only endorse items that they have never seen, but they also make subjective judgments that they remember experiencing them. This is a difficult problem for most dual process models of recognition memory, as they propose that false memories should be based on familiarity, not recollection. We present a new computational model of recollection based on the recognition through semantic synchronization model of Johns, Jones, \& Mewhort (2012), and fuzzy trace theory (Brainerd \& Reyna, 2002). In addition to standard and false recognition, the model successfully explains multiple studies on both true and false recollection.},
	author = {Johns, Brendan T and Jones, Michael N and Mewhort, Douglas J K},
	langid = {english},
	file = {PDF:/home/connorh/Zotero/storage/EQULWL28/Johns et al. - A Continuous Source Reinstatement Model of True and Illusory Recollection.pdf:application/pdf},
}

@misc{wu_uniform_2024,
	title = {Uniform Memory Retrieval with Larger Capacity for Modern Hopfield Models},
	url = {http://arxiv.org/abs/2404.03827},
	doi = {10.48550/arXiv.2404.03827},
	abstract = {We propose a two-stage memory retrieval dynamics for modern Hopfield models, termed \${\textbackslash}mathtt\{U{\textbackslash}text\{-\}Hop\}\$, with enhanced memory capacity. Our key contribution is a learnable feature map \${\textbackslash}Phi\$ which transforms the Hopfield energy function into kernel space. This transformation ensures convergence between the local minima of energy and the fixed points of retrieval dynamics within the kernel space. Consequently, the kernel norm induced by \${\textbackslash}Phi\$ serves as a novel similarity measure. It utilizes the stored memory patterns as learning data to enhance memory capacity across all modern Hopfield models. Specifically, we accomplish this by constructing a separation loss \${\textbackslash}mathcal\{L\}\_{\textbackslash}Phi\$ that separates the local minima of kernelized energy by separating stored memory patterns in kernel space. Methodologically, \${\textbackslash}mathtt\{U{\textbackslash}text\{-\}Hop\}\$ memory retrieval process consists of: (Stage I) minimizing separation loss for a more uniform memory (local minimum) distribution, followed by (Stage {II}) standard Hopfield energy minimization for memory retrieval. This results in a significant reduction of possible metastable states in the Hopfield energy function, thus enhancing memory capacity by preventing memory confusion. Empirically, with real-world datasets, we demonstrate that \${\textbackslash}mathtt\{U{\textbackslash}text\{-\}Hop\}\$ outperforms all existing modern Hopfield models and state-of-the-art similarity measures, achieving substantial improvements in both associative memory retrieval and deep learning tasks. Code is available at https://github.com/{MAGICS}-{LAB}/{UHop} ; future updates are on {arXiv}:2404.03827},
	number = {{arXiv}:2404.03827},
	publisher = {{arXiv}},
	author = {Wu, Dennis and Hu, Jerry Yao-Chieh and Hsiao, Teng-Yun and Liu, Han},
	urldate = {2025-09-15},
	date = {2024-11-10},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2404.03827 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {PDF:/home/connorh/Zotero/storage/N9CEUDN4/Wu et al. - 2024 - Uniform Memory Retrieval with Larger Capacity for Modern Hopfield Models.pdf:application/pdf},
}

@article{johnson_spiking_nodate,
	title = {A {SPIKING} {BIDIRECTIONAL} {ASSOCIATIVE} {MEMORY} {NEURAL} {NETWORK}},
	abstract = {Spiking neural networks ({SNNs}) are a more biologically realistic model of the brain than traditional analog neural networks and therefore should be better for modelling certain functions of the human brain. This thesis uses the concept of deriving an {SNN} from an accepted nonspiking neural network via analysis and modifications of the transmission function. We investigate this process to determine if and how the modifications can be made to minimize loss of information during the transition from non-spiking to spiking while retaining positive features and functionality of the non-spiking network. By comparing combinations of spiking neuron models and networks against each other, we determined that replacing the transmission function with a neural model that is similar to it allows for the easiest method to create a spiking neural network that works comparatively well. This similarity between transmission function and neuron model allows for easier parameter selection which is a key component in getting a functioning {SNN}. The parameters all play different roles, but for the most part, parameters that speed up spiking, such as large resistance values or small rheobases generally help the accuracy of the network. But the network is still incomplete for a spiking neural network since this conversion is often only performed after learning has been completed in analog form. The neuron model and subsequent network developed here are the initial steps in creating a bidirectional {SNN} that handles hetero-associative and auto-associative recall and can be switched easily between spiking and non-spiking with minimal to no loss of data. By tying everything to the transmission function, the non-spiking learning rule, which in our case uses the transmission function, and the neural model of the {SNN}, we are able to create a functioning {SNN}. Without this similarity, we find that creating {SNN} are much more complicated and require much more work in parameter optimization to achieve a functioning {SNN}.},
	author = {Johnson, Melissa},
	langid = {english},
	file = {PDF:/home/connorh/Zotero/storage/NLPHKY3H/Johnson - A SPIKING BIDIRECTIONAL ASSOCIATIVE MEMORY NEURAL NETWORK.pdf:application/pdf},
}

@article{tang_recurrent_2023,
	title = {Recurrent predictive coding models for associative memory employing covariance learning},
	volume = {19},
	issn = {1553-7358},
	url = {https://dx.plos.org/10.1371/journal.pcbi.1010719},
	doi = {10.1371/journal.pcbi.1010719},
	abstract = {The computational principles adopted by the hippocampus in associative memory ({AM}) tasks have been one of the most studied topics in computational and theoretical neuroscience. Recent theories suggested that {AM} and the predictive activities of the hippocampus could be described within a unitary account, and that predictive coding underlies the computations supporting {AM} in the hippocampus. Following this theory, a computational model based on classical hierarchical predictive networks was proposed and was shown to perform well in various {AM} tasks. However, this fully hierarchical model did not incorporate recurrent connections, an architectural component of the {CA}3 region of the hippocampus that is crucial for {AM}. This makes the structure of the model inconsistent with the known connectivity of {CA}3 and classical recurrent models such as Hopfield Networks, which learn the covariance of inputs through their recurrent connections to perform {AM}. Earlier {PC} models that learn the covariance information of inputs explicitly via recurrent connections seem to be a solution to these issues. Here, we show that although these models can perform {AM}, they do it in an implausible and numerically unstable way. Instead, we propose alternatives to these earlier covariance-learning predictive coding networks, which learn the covariance information implicitly and plausibly, and can use dendritic structures to encode prediction errors. We show analytically that our proposed models are perfectly equivalent to the earlier predictive coding model learning covariance explicitly, and encounter no numerical issues when performing {AM} tasks in practice. We further show that our models can be combined with hierarchical predictive coding networks to model the hippocampo-neocortical interactions. Our models provide a biologically plausible approach to modelling the hippocampal network, pointing to a potential computational mechanism during hippocampal memory formation and recall, which employs both predictive coding and covariance learning based on the recurrent network structure of the hippocampus.},
	pages = {e1010719},
	number = {4},
	journaltitle = {{PLOS} Computational Biology},
	shortjournal = {{PLoS} Comput Biol},
	author = {Tang, Mufeng and Salvatori, Tommaso and Millidge, Beren and Song, Yuhang and Lukasiewicz, Thomas and Bogacz, Rafal},
	editor = {Wei, Xue-Xin},
	urldate = {2025-09-15},
	date = {2023-04-14},
	langid = {english},
	file = {PDF:/home/connorh/Zotero/storage/YXPMMKU4/Tang et al. - 2023 - Recurrent predictive coding models for associative memory employing covariance learning.pdf:application/pdf},
}

@article{chartier_ndram_2005,
	title = {{NDRAM}: Nonlinear Dynamic Recurrent Associative Memory for Learning Bipolar and Nonbipolar Correlated Patterns},
	volume = {16},
	rights = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/{IEEE}.html},
	issn = {1045-9227},
	url = {http://ieeexplore.ieee.org/document/1528519/},
	doi = {10.1109/TNN.2005.852861},
	shorttitle = {{NDRAM}},
	abstract = {This paper presents a new unsupervised attractor neural network, which, contrary to optimal linear associative memory models, is able to develop nonbipolar attractors as well as bipolar attractors. Moreover, the model is able to develop less spurious attractors and has a better recall performance under random noise than any other Hopﬁeld type neural network. Those performances are obtained by a simple Hebbian/anti-Hebbian online learning rule that directly incorporates feedback from a speciﬁc nonlinear transmission rule. Several computer simulations show the model’s distinguishing properties.},
	pages = {1393--1400},
	number = {6},
	journaltitle = {{IEEE} Transactions on Neural Networks},
	shortjournal = {{IEEE} Trans. Neural Netw.},
	author = {Chartier, S. and Proulx, R.},
	urldate = {2025-09-15},
	date = {2005-11},
	langid = {english},
	file = {PDF:/home/connorh/Zotero/storage/2NTJ3AQK/Chartier and Proulx - 2005 - NDRAM Nonlinear Dynamic Recurrent Associative Memory for Learning Bipolar and Nonbipolar Correlated.pdf:application/pdf},
}

@misc{krotov_large_2021,
	title = {Large Associative Memory Problem in Neurobiology and Machine Learning},
	url = {http://arxiv.org/abs/2008.06996},
	doi = {10.48550/arXiv.2008.06996},
	abstract = {Dense Associative Memories or modern Hopﬁeld networks permit storage and reliable retrieval of an exponentially large (in the dimension of feature space) number of memories. At the same time, their naive implementation is non-biological, since it seemingly requires the existence of many-body synaptic junctions between the neurons. We show that these models are effective descriptions of a more microscopic (written in terms of biological degrees of freedom) theory that has additional (hidden) neurons and only requires two-body interactions between them. For this reason our proposed microscopic theory is a valid model of large associative memory with a degree of biological plausibility. The dynamics of our network and its reduced dimensional equivalent both minimize energy (Lyapunov) functions. When certain dynamical variables (hidden neurons) are integrated out from our microscopic theory, one can recover many of the models that were previously discussed in the literature, e.g. the model presented in “Hopﬁeld Networks is All You Need” paper. We also provide an alternative derivation of the energy function and the update rule proposed in the aforementioned paper and clarify the relationships between various models of this class.},
	number = {{arXiv}:2008.06996},
	publisher = {{arXiv}},
	author = {Krotov, Dmitry and Hopfield, John},
	urldate = {2025-09-15},
	date = {2021-04-27},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2008.06996 [q-bio]},
	keywords = {Quantitative Biology - Neurons and Cognition, Computer Science - Computation and Language, Computer Science - Machine Learning, Statistics - Machine Learning, Condensed Matter - Disordered Systems and Neural Networks},
	file = {PDF:/home/connorh/Zotero/storage/ZWYE47U5/Krotov and Hopfield - 2021 - Large Associative Memory Problem in Neurobiology and Machine Learning.pdf:application/pdf},
}

@article{xie_equivalence_2003,
	title = {Equivalence of Backpropagation and Contrastive Hebbian Learning in a Layered Network},
	volume = {15},
	issn = {0899-7667, 1530-888X},
	url = {https://direct.mit.edu/neco/article/15/2/441-454/6701},
	doi = {10.1162/089976603762552988},
	abstract = {Backpropagation and contrastive Hebbian learning are two methods of training networks with hidden neurons. Backpropagation computes an error signal for the output neurons and spreads it over the hidden neurons. Contrastive Hebbian learning involves clamping the output neurons at desired values and letting the effect spread through feedback connections over the entire network. To investigate the relationship between these two forms of learning, we consider a special case in which they are identical: a multilayer perceptron with linear output units, to which weak feedback connections have been added. In this case, the change in network state caused by clamping the output neurons turns out to be the same as the error signal spread by backpropagation, except for a scalar prefactor. This suggests that the functionality of backpropagation can be realized alternatively by a Hebbian-type learning algorithm, which is suitable for implementation in biological networks.},
	pages = {441--454},
	number = {2},
	journaltitle = {Neural Computation},
	shortjournal = {Neural Computation},
	author = {Xie, Xiaohui and Seung, H. Sebastian},
	urldate = {2025-09-15},
	date = {2003-02-01},
	langid = {english},
	file = {PDF:/home/connorh/Zotero/storage/MB4K5RQL/Xie and Seung - 2003 - Equivalence of Backpropagation and Contrastive Hebbian Learning in a Layered Network.pdf:application/pdf},
}

@misc{salvatori_associative_2021,
	title = {Associative Memories via Predictive Coding},
	url = {http://arxiv.org/abs/2109.08063},
	doi = {10.48550/arXiv.2109.08063},
	abstract = {Associative memories in the brain receive and store patterns of activity registered by the sensory neurons, and are able to retrieve them when necessary. Due to their importance in human intelligence, computational models of associative memories have been developed for several decades now. They include autoassociative memories, which allow for storing data points and retrieving a stored data point s when provided with a noisy or partial variant of s, and heteroassociative memories, able to store and recall multi-modal data. In this paper, we present a novel neural model for realizing associative memories, based on a hierarchical generative network that receives external stimuli via sensory neurons. This model is trained using predictive coding, an error-based learning algorithm inspired by information processing in the cortex. To test the capabilities of this model, we perform multiple retrieval experiments from both corrupted and incomplete data points. In an extensive comparison, we show that this new model outperforms in retrieval accuracy and robustness popular associative memory models, such as autoencoders trained via backpropagation, and modern Hopﬁeld networks. In particular, in completing partial data points, our model achieves remarkable results on natural image datasets, such as {ImageNet}, with a surprisingly high accuracy, even when only a tiny fraction of pixels of the original images is presented. Furthermore, we show that this method is able to handle multi-modal data, retrieving images from descriptions, and vice versa. We conclude by discussing the possible impact of this work in the neuroscience community, by showing that our model provides a plausible framework to study learning and retrieval of memories in the brain, as it closely mimics the behavior of the hippocampus as a memory index and generative model.},
	number = {{arXiv}:2109.08063},
	publisher = {{arXiv}},
	author = {Salvatori, Tommaso and Song, Yuhang and Hong, Yujian and Frieder, Simon and Sha, Lei and Xu, Zhenghua and Bogacz, Rafal and Lukasiewicz, Thomas},
	urldate = {2025-09-15},
	date = {2021-09-16},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2109.08063 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {PDF:/home/connorh/Zotero/storage/5T8FDEUA/Salvatori et al. - 2021 - Associative Memories via Predictive Coding.pdf:application/pdf},
}

@misc{salvatori_associative_2024,
	title = {Associative Memories in the Feature Space},
	url = {http://arxiv.org/abs/2402.10814},
	doi = {10.48550/arXiv.2402.10814},
	abstract = {An autoassociative memory model is a function that, given a set of data points, takes as input an arbitrary vector and outputs the most similar data point from the memorized set. However, popular memory models fail to retrieve images even when the corruption is mild and easy to detect for a human evaluator. This is because similarities are evaluated in the raw pixel space, which does not contain any semantic information about the images. This problem can be easily solved by computing similarities in an embedding space instead of the pixel space. We show that an effective way of computing such embeddings is via a network pretrained with a contrastive loss. As the dimension of embedding spaces is often significantly smaller than the pixel space, we also have a faster computation of similarity scores. We test this method on complex datasets such as {CIFAR}10 and {STL}10. An additional drawback of current models is the need of storing the whole dataset in the pixel space, which is often extremely large. We relax this condition and propose a class of memory models that only stores low-dimensional semantic embeddings, and uses them to retrieve similar, but not identical, memories. We demonstrate a proof of concept of this method on a simple task on the {MNIST} dataset.},
	number = {{arXiv}:2402.10814},
	publisher = {{arXiv}},
	author = {Salvatori, Tommaso and Millidge, Beren and Song, Yuhang and Bogacz, Rafal and Lukasiewicz, Thomas},
	urldate = {2025-09-15},
	date = {2024-02-16},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2402.10814 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	file = {PDF:/home/connorh/Zotero/storage/EMU7UIW5/Salvatori et al. - 2024 - Associative Memories in the Feature Space.pdf:application/pdf},
}

@article{chartier_bidirectional_2006,
	title = {A Bidirectional Heteroassociative Memory for Binary and Grey-Level Patterns},
	volume = {17},
	rights = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/{IEEE}.html},
	issn = {1045-9227},
	url = {http://ieeexplore.ieee.org/document/1603624/},
	doi = {10.1109/TNN.2005.863420},
	abstract = {Typical bidirectional associative memories ({BAM}) use an ofﬂine, one-shot learning rule, have poor memory storage capacity, are sensitive to noise, and are subject to spurious steady states during recall. Recent work on {BAM} has improved network performance in relation to noisy recall and the number of spurious attractors, but at the cost of an increase in {BAM} complexity. In all cases, the networks can only recall bipolar stimuli and, thus, are of limited use for grey-level pattern recall. In this paper, we introduce a new bidirectional heteroassociative memory model that uses a simple self-convergent iterative learning rule and a new nonlinear output function. As a result, the model can learn online without being subject to overlearning. Our simulation results show that this new model causes fewer spurious attractors when compared to others popular {BAM} networks, for a comparable performance in terms of tolerance to noise and storage capacity. In addition, the novel output function enables it to learn and recall grey-level patterns in a bidirectional way.},
	pages = {385--396},
	number = {2},
	journaltitle = {{IEEE} Transactions on Neural Networks},
	shortjournal = {{IEEE} Trans. Neural Netw.},
	author = {Chartier, S. and Boukadoum, M.},
	urldate = {2025-09-15},
	date = {2006-03},
	langid = {english},
	file = {PDF:/home/connorh/Zotero/storage/3JF6CICH/Chartier and Boukadoum - 2006 - A Bidirectional Heteroassociative Memory for Binary and Grey-Level Patterns.pdf:application/pdf},
}

@article{arthur_s_reber_implicit_1993,
	title = {Implicit learning and tacit knowledge},
	abstract = {I examine the phenomenon of implicit learning, the process by which knowledge about the ralegoverned complexities of the stimulus environment is acquired independently of conscious attempts to do so. Our research with the two, seemingly disparate experimental paradigms of synthetic grammar learning and probability learning is reviewed and integrated with other approaches to the general problem of unconscious cognition. The conclusions reached are as follows: (a) Implicit learning produces a tacit knowledge base that is abstract and representative of the structure of the environment; (b) such knowledge is optimally acquired independently of conscious efforts to learn; and (c) it can be used implicitly to solve problems and make accurate decisions about novel stimulus circumstances. Various epistemological issues and related prob1 lems such as intuition, neuroclinical disorders of learning and memory, and the relationship of evolutionary processes to cognitive science are also discussed. Some two decades ago the term implicit learning was first used to characterize how one develops intuitive knowledge about the underlying structure of a complex stimulus environment (Reber, 1965, 1967). In those early writings, I argued that implicit learning is characterized by two critical features: (a) It is an unconscious process and (b) it yields abstract knowledge. Implicit knowledge results from the induction of an abstract representation of the structure that the stimulus environment displays, and this knowledge is acquired in the absence of conscious, reflective strategies to learn. Since then, the evidence in support of this theory has been abundant, and many of the details of the process have been sharpened. This article is an overview of this evidence and an attempt to extend the general concepts to provide some insight into a variety of related processes such as arriving at intuitive judgments, complex decision making, and, in a broad sense, learning about the complex covariations among events that characterize the environment. Put simply, this is an article about learning. It seems curious, given the pattern of psychological investigation of the middle decades of this century, that the topic of learning should be so poorly represented in the contemporary literature in cognitive psychology. The energies of cognitive scientists have been invested largely in the analysis and modeling of existing knowledge rather than in investigations of how it was acquired. For example, in an important recent article on the general topic of unconscious memorial systems, Schacter (1987) never came to grips with the distinction between implicit learning and implicit memory. The latter, the focus of his review, was dealt with historically, characterized, out},
	author = {{Arthur S. Reber} and Reber, Arthur S.},
	date = {1993-01-01},
	note = {{MAG} {ID}: 1967658564},
}

@article{darryl_w_schneider_memory-based_2011,
	title = {A memory-based model of Hick's law.},
	volume = {62},
	doi = {10.1016/j.cogpsych.2010.11.001},
	abstract = {Abstract   We propose and evaluate a memory-based model of Hick’s law, the approximately linear increase in choice reaction time with the logarithm of set size (the number of stimulus–response alternatives). According to the model, Hick’s law reflects a combination of associative interference during retrieval from declarative memory and occasional savings for stimulus–response repetitions due to non-retrieval. Fits to existing data sets show that the model accounts for the basic set-size effect, changes in the set-size effect with practice, and stimulus–response-repetition effects that challenge the information-theoretic view of Hick’s law. We derive the model’s prediction of an interaction between set size, stimulus fan (the number of responses associated with a particular stimulus), and stimulus–response transition, which is subsequently tested and confirmed in two experiments. Collectively, the results support the core structure of the model and its explanation of Hick’s law in terms of basic memory effects.},
	pages = {193--222},
	number = {3},
	journaltitle = {Cognitive Psychology},
	author = {{Darryl W. Schneider} and Schneider, Darryl W. and {John R. Anderson} and Anderson, John R.},
	date = {2011-05-01},
	doi = {10.1016/j.cogpsych.2010.11.001},
	pmcid = {3031137},
	pmid = {21293788},
	note = {{MAG} {ID}: 2165934783},
}

@article{allan_collins_spreading-activation_1975,
	title = {A spreading-activation theory of semantic processing},
	volume = {82},
	doi = {10.1037/0033-295x.82.6.407},
	abstract = {This paper presents a spreading-acti vation theory of human semantic processing, which can be applied to a wide range of recent experimental results. The theory is based on Quillian's theory of semantic memory search and semantic preparation, or priming. In conjunction with this, several of the miscondeptions concerning Qullian's theory are discussed. A number of additional assumptions are proposed for his theory in order to apply it to recent experiments. The present paper shows how the extended theory can account for results of several production experiments by Loftus, Juola and Atkinson's multiple-category experiment, Conrad's sentence-verification experiments, and several categorization experiments on the effect of semantic relatedness and typicality by Holyoak and Glass, Rips, Shoben, and Smith, and Rosch. The paper also provides a critique of the Smith, Shoben, and Rips model for categorization judgments. Some years ago, Quillian1 (1962, 1967) proposed a spreading-acti vation theory of human semantic processing that he tried to implement in computer simulations of memory search (Quillian, 1966) and comprehension (Quillian, 1969). The theory viewed memory search as activation spreading from two or more concept nodes in a semantic network until an intersection was found. The effects of preparation (or priming) in semantic memory were also explained in terms of spreading activation from the node of the primed concept. Rather than a theory to explain data, it was a theory designed to show how to build human semantic structure and processing into a computer.},
	pages = {407--428},
	number = {6},
	journaltitle = {Psychological Review},
	author = {{Allan Collins} and Collins, Allan and {Elizabeth F. Loftus} and Loftus, Elizabeth F.},
	date = {1975-11-01},
	doi = {10.1037/0033-295x.82.6.407},
	note = {{MAG} {ID}: 2135255848},
}

@article{john_r_anderson_fan_1999,
	title = {The fan effect: New results and new theories.},
	volume = {128},
	doi = {10.1037/0096-3445.128.2.186},
	abstract = {The fan effect (J. R. Anderson, 1974) has been attributed to interference among competing associations to a concept. Recently, it has been suggested that such effects might be due to multiple mental models (G. A. Radvansky. D. H. Spieler, \& R. T. Zacks, 1993) or suppression of concepts (M. C. Anderson \& B. A. Spellman, 1995; A. R. A. Conway \& R. W. Engle, 1994). It was found that the Adaptive Control of Thought-Rational ({ACT}-R) theory, which embodies associative interference, is consistent with the results of G. A. Radvansky et al. and that there is no evidence for concept suppression in a new fan experiment. The {ACT}-R model provides good quantitative fits to the results, as shown in a variety of experiments. The 3 key concepts in these fits are (a) the associative strength between 2 concepts reflects the degree to which one concept predicts the other; (b) foils are rejected by retrieving mismatching facts; and (c) participants can adjust the relative weights they give to various cues in retrieval.},
	pages = {186--197},
	number = {2},
	journaltitle = {Journal of Experimental Psychology: General},
	author = {{John R. Anderson} and Anderson, John R. and {Lynne M. Reder} and Reder, Lynne M.},
	date = {1999-06-01},
	doi = {10.1037/0096-3445.128.2.186},
	note = {{MAG} {ID}: 2104171182},
}

@article{sepp_hochreiter_long_1997,
	title = {Long short-term memory},
	volume = {9},
	doi = {10.1162/neco.1997.9.8.1735},
	abstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory ({LSTM}). Truncating the gradient where this does not do harm, {LSTM} can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. {LSTM} is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, {LSTM} leads to many more successful runs, and learns much faster. {LSTM} also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.},
	pages = {1735--1780},
	number = {8},
	journaltitle = {Neural Computation},
	author = {{Sepp Hochreiter} and Hochreiter, Sepp and {Jürgen Schmidhuber} and Schmidhuber, Jürgen},
	date = {1997-11-01},
	doi = {10.1162/neco.1997.9.8.1735},
	pmid = {9377276},
	note = {{MAG} {ID}: 2064675550},
}

@article{robert_j_mceliece_capacity_1987,
	title = {The capacity of the Hopfield associative memory},
	volume = {33},
	doi = {10.1109/tit.1987.1057328},
	abstract = {Techniques from coding theory are applied to study rigorously the capacity of the Hopfield associative memory. Such a memory stores n -tuple of {\textbackslash}pm 1 's. The components change depending on a hard-limited version of linear functions of all other components. With symmetric connections between components, a stable state is ultimately reached. By building up the connection matrix as a sum-of-outer products of m fundamental memories, one hopes to be able to recover a certain one of the m memories by using an initial n -tuple probe vector less than a Hamming distance n/2 away from the fundamental memory. If m fundamental memories are chosen at random, the maximum asympotic value of m in order that most of the m original memories are exactly recoverable is n/(2 {\textbackslash}log n) . With the added restriction that every one of the m fundamental memories be recoverable exactly, m can be no more than n/(4 {\textbackslash}log n) asymptotically as n approaches infinity. Extensions are also considered, in particular to capacity under quantization of the outer-product connection matrix. This quantized memory capacity problem is closely related to the capacity of the quantized Gaussian channel.},
	pages = {461--482},
	number = {4},
	journaltitle = {{IEEE} Transactions on Information Theory},
	author = {{Robert J. McEliece} and McEliece, Robert J. and {Edward C. Posner} and Posner, Edward C. and {E. R. Rodemich} and Rodemich, Eugene R. and {Santosh S. Venkatesh} and {Santosh S. Venkatesh} and Venkatesh, Santosh S.},
	date = {1987-07-01},
	doi = {10.1109/tit.1987.1057328},
	note = {{MAG} {ID}: 1984431842},
	file = {PDF:/home/connorh/Zotero/storage/URWUVLTY/McEliece et al. - 1987 - The capacity of the Hopfield associative memory.pdf:application/pdf},
}

@article{dmitry_krotov_dense_2016,
	title = {Dense Associative Memory for Pattern Recognition},
	volume = {29},
	abstract = {A model of associative memory is studied, which stores and reliably retrieves many more patterns than the number of neurons in the network. We propose a simple duality between this dense associative memory and neural networks commonly used in deep learning. On the associative memory side of this duality, a family of models that smoothly interpolates between two limiting cases can be constructed. One limit is referred to as the feature-matching mode of pattern recognition, and the other one as the prototype regime. On the deep learning side of the duality, this family corresponds to feedforward neural networks with one hidden layer and various activation functions, which transmit the activities of the visible neurons to the hidden layer. This family of activation functions includes logistics, rectified linear units, and rectified polynomials of higher degrees. The proposed duality makes it possible to apply energy-based intuition from associative memory to analyze computational properties of neural networks with unusual activation functions - the higher rectified polynomials which until now have not been used in deep learning. The utility of the dense memories is illustrated for two test cases: the logical gate {XOR} and the recognition of handwritten digits from the {MNIST} data set.},
	pages = {1172--1180},
	author = {{Dmitry Krotov} and Krotov, Dmitry and {J. J. Hopfield} and Hopfield, John J.},
	date = {2016-06-01},
	note = {{MAG} {ID}: 2962940432},
}

@misc{frady_computing_2021,
	title = {Computing on Functions Using Randomized Vector Representations},
	url = {http://arxiv.org/abs/2109.03429},
	doi = {10.48550/arXiv.2109.03429},
	abstract = {Vector space models for symbolic processing that encode symbols by random vectors have been proposed in cognitive science and connectionist communities under the names Vector Symbolic Architecture ({VSA}), and, synonymously, Hyperdimensional ({HD}) computing. In this paper, we generalize {VSAs} to function spaces by mapping continuous-valued data into a vector space such that the inner product between the representations of any two data points represents a similarity kernel. By analogy to {VSA}, we call this new function encoding and computing framework Vector Function Architecture ({VFA}). In {VFAs}, vectors can represent individual data points as well as elements of a function space (a reproducing kernel Hilbert space). The algebraic vector operations, inherited from {VSA}, correspond to well-deﬁned operations in function space. Furthermore, we study a previously proposed method for encoding continuous data, fractional power encoding ({FPE}), which uses exponentiation of a random base vector to produce randomized representations of data points and fulﬁlls the kernel properties for inducing a {VFA}. We show that the distribution from which elements of the base vector are sampled determines the shape of the {FPE} kernel, which in turn induces a {VFA} for computing with band-limited functions. In particular, {VFAs} provide an algebraic framework for implementing large-scale kernel machines with random features, extending Rahimi and Recht (2007). Finally, we demonstrate several applications of {VFA} models to problems in image recognition, density estimation and nonlinear regression. Our analyses and results suggest that {VFAs} constitute a powerful new framework for representing and manipulating functions in distributed neural systems, with myriad applications in artiﬁcial intelligence.},
	number = {{arXiv}:2109.03429},
	publisher = {{arXiv}},
	author = {Frady, E. Paxon and Kleyko, Denis and Kymn, Christopher J. and Olshausen, Bruno A. and Sommer, Friedrich T.},
	urldate = {2025-08-28},
	date = {2021-09-08},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2109.03429 [cs]},
	keywords = {Computer Science - Neural and Evolutionary Computing, Quantitative Biology - Neurons and Cognition, Computer Science - Machine Learning},
	file = {PDF:/home/connorh/Zotero/storage/P67P8DE3/Frady et al. - 2021 - Computing on Functions Using Randomized Vector Representations.pdf:application/pdf},
}

@misc{pham_memorization_2025,
	title = {Memorization to Generalization: Emergence of Diffusion Models from Associative Memory},
	url = {http://arxiv.org/abs/2505.21777},
	doi = {10.48550/arXiv.2505.21777},
	shorttitle = {Memorization to Generalization},
	abstract = {Hopfield networks are associative memory ({AM}) systems, designed for storing and retrieving patterns as local minima of an energy landscape. In the classical Hopfield model, an interesting phenomenon occurs when the amount of training data reaches its critical memory load — spurious states, or unintended stable points, emerge at the end of the retrieval dynamics, leading to incorrect recall. In this work, we examine diffusion models, commonly used in generative modeling, from the perspective of {AMs}. The training phase of diffusion model is conceptualized as memory encoding (training data is stored in the memory). The generation phase is viewed as an attempt of memory retrieval. In the small data regime the diffusion model exhibits a strong memorization phase, where the network creates distinct basins of attraction around each sample in the training set, akin to the Hopfield model below the critical memory load. In the large data regime, a different phase appears where an increase in the size of the training set fosters the creation of new attractor states that correspond to manifolds of the generated samples. Spurious states appear at the boundary of this transition and correspond to emergent attractor states, which are absent in the training set, but, at the same time, have distinct basins of attraction around them. Our findings provide: a novel perspective on the memorization-generalization phenomenon in diffusion models via the lens of {AMs}, theoretical prediction of existence of spurious states, empirical validation of this prediction in commonly-used diffusion models.},
	number = {{arXiv}:2505.21777},
	publisher = {{arXiv}},
	author = {Pham, Bao and Raya, Gabriel and Negri, Matteo and Zaki, Mohammed J. and Ambrogioni, Luca and Krotov, Dmitry},
	urldate = {2025-08-28},
	date = {2025-06-20},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2505.21777 [cs]},
	keywords = {Quantitative Biology - Neurons and Cognition, Computer Science - Machine Learning, Statistics - Machine Learning, Condensed Matter - Disordered Systems and Neural Networks, Computer Science - Computer Vision and Pattern Recognition},
	file = {PDF:/home/connorh/Zotero/storage/JZMHQKGP/Pham et al. - 2025 - Memorization to Generalization Emergence of Diffusion Models from Associative Memory.pdf:application/pdf},
}

@book{mcclelland_parallel_1986,
	title = {Parallel Distributed Processing, Volume 2: Explorations in the Microstructure of Cognition: Psychological and Biological Models},
	isbn = {978-0-262-29126-2},
	url = {https://doi.org/10.7551/mitpress/5237.001.0001},
	abstract = {What makes people smarter than computers? These volumes by a pioneering neurocomputing group suggest that the answer lies in the massively parallel architecture of the human mind. They describe a new theory of cognition called connectionism that is challenging the idea of symbolic computation that has traditionally been at the center of debate in theoretical discussions about the mind. The authors' theory assumes the mind is composed of a great number of elementary units connected in a neural network. Mental processes are interactions between these units which excite and inhibit each other in parallel rather than sequential operations. In this context, knowledge can no longer be thought of as stored in localized structures; instead, it consists of the connections between pairs of units that are distributed throughout the network. Volume 1 lays the foundations of this exciting theory of parallel distributed processing, while Volume 2 applies it to a number of specific issues in cognitive science and neuroscience, with chapters describing models of aspects of perception, memory, language, and thought.Bradford Books imprint},
	publisher = {The {MIT} Press},
	author = {McClelland, James L. and Rumelhart, David E. and {PDP Research Group}},
	urldate = {2025-08-28},
	date = {1986-07-17},
	doi = {10.7551/mitpress/5237.001.0001},
}

@book{rumelhart_parallel_1986,
	title = {Parallel Distributed Processing, Volume 1: Explorations in the Microstructure of Cognition: Foundations},
	isbn = {978-0-262-29140-8},
	url = {https://doi.org/10.7551/mitpress/5236.001.0001},
	abstract = {What makes people smarter than computers? These volumes by a pioneering neurocomputing group suggest that the answer lies in the massively parallel architecture of the human mind. They describe a new theory of cognition called connectionism that is challenging the idea of symbolic computation that has traditionally been at the center of debate in theoretical discussions about the mind. The authors' theory assumes the mind is composed of a great number of elementary units connected in a neural network. Mental processes are interactions between these units which excite and inhibit each other in parallel rather than sequential operations. In this context, knowledge can no longer be thought of as stored in localized structures; instead, it consists of the connections between pairs of units that are distributed throughout the network. Volume 1 lays the foundations of this exciting theory of parallel distributed processing, while Volume 2 applies it to a number of specific issues in cognitive science and neuroscience, with chapters describing models of aspects of perception, memory, language, and thought.Bradford Books imprint},
	publisher = {The {MIT} Press},
	author = {Rumelhart, David E. and McClelland, James L. and {PDP Research Group}},
	urldate = {2025-08-28},
	date = {1986-07-17},
	doi = {10.7551/mitpress/5236.001.0001},
}

@article{gerstner_hebbian_nodate,
	title = {Hebbian Learning and Plasticity},
	author = {Gerstner, Wulfram},
	langid = {english},
	file = {PDF:/home/connorh/Zotero/storage/H8T9KHF6/Gerstner - Hebbian Learning and Plasticity.pdf:application/pdf},
}

@article{gerstner_mathematical_2002,
	title = {Mathematical formulations of Hebbian learning},
	volume = {87},
	rights = {http://www.springer.com/tdm},
	issn = {03401200},
	url = {http://link.springer.com/10.1007/s00422-002-0353-y},
	doi = {10.1007/s00422-002-0353-y},
	pages = {404--415},
	number = {5},
	journaltitle = {Biological Cybernetics},
	author = {Gerstner, Wulfram and Kistler, Werner M.},
	urldate = {2025-09-25},
	date = {2002-12-01},
	langid = {english},
	file = {PDF:/home/connorh/Zotero/storage/4YVF2C2F/Gerstner and Kistler - 2002 - Mathematical formulations of Hebbian learning.pdf:application/pdf},
}

@article{iatropoulos_kernel_nodate,
	title = {Kernel Memory Networks: A Unifying Framework for Memory Modeling},
	abstract = {We consider the problem of training a neural network to store a set of patterns with maximal noise robustness. A solution, in terms of optimal weights and state update rules, is derived by training each individual neuron to perform either kernel classiﬁcation or interpolation with a minimum weight norm. By applying this method to feed-forward and recurrent networks, we derive optimal models, termed kernel memory networks, that include, as special cases, many of the hetero- and auto-associative memory models that have been proposed over the past years, such as modern Hopﬁeld networks and Kanerva’s sparse distributed memory. We modify Kanerva’s model and demonstrate a simple way to design a kernel memory network that can store an exponential number of continuous-valued patterns with a ﬁnite basin of attraction. The framework of kernel memory networks offers a simple and intuitive way to understand the storage capacity of previous memory models, and allows for new biological interpretations in terms of dendritic non-linearities and synaptic cross-talk.},
	author = {Iatropoulos, Georgios and Brea, Johanni and Gerstner, Wulfram},
	langid = {english},
	file = {PDF:/home/connorh/Zotero/storage/NFRM3LLB/Iatropoulos et al. - Kernel Memory Networks A Unifying Framework for Memory Modeling.pdf:application/pdf},
}

@misc{chaudhry_long_2023,
	title = {Long Sequence Hopfield Memory},
	url = {http://arxiv.org/abs/2306.04532},
	doi = {10.48550/arXiv.2306.04532},
	abstract = {Sequence memory is an essential attribute of natural and artificial intelligence that enables agents to encode, store, and retrieve complex sequences of stimuli and actions. Computational models of sequence memory have been proposed where recurrent Hopfield-like neural networks are trained with temporally asymmetric Hebbian rules. However, these networks suffer from limited sequence capacity (maximal length of the stored sequence) due to interference between the memories. Inspired by recent work on Dense Associative Memories, we expand the sequence capacity of these models by introducing a nonlinear interaction term, enhancing separation between the patterns. We derive novel scaling laws for sequence capacity with respect to network size, significantly outperforming existing scaling laws for models based on traditional Hopfield networks, and verify these theoretical results with numerical simulation. Moreover, we introduce a generalized pseudoinverse rule to recall sequences of highly correlated patterns. Finally, we extend this model to store sequences with variable timing between states’ transitions and describe a biologically-plausible implementation, with connections to motor neuroscience.},
	number = {{arXiv}:2306.04532},
	publisher = {{arXiv}},
	author = {Chaudhry, Hamza Tahir and Zavatone-Veth, Jacob A. and Krotov, Dmitry and Pehlevan, Cengiz},
	urldate = {2025-10-02},
	date = {2023-11-02},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2306.04532 [cs]},
	keywords = {Computer Science - Neural and Evolutionary Computing, Quantitative Biology - Neurons and Cognition, Computer Science - Machine Learning, Statistics - Machine Learning, Condensed Matter - Disordered Systems and Neural Networks},
	file = {PDF:/home/connorh/Zotero/storage/E5KKD4VR/Chaudhry et al. - 2023 - Long Sequence Hopfield Memory.pdf:application/pdf},
}

@article{little_analytic_1978,
	title = {Analytic study of the memory storage capacity of a neural network},
	volume = {39},
	rights = {https://www.elsevier.com/tdm/userlicense/1.0/},
	issn = {00255564},
	url = {https://linkinghub.elsevier.com/retrieve/pii/0025556478900585},
	doi = {10.1016/0025-5564(78)90058-5},
	abstract = {Previously, we developed a model of short and long term memory which was based on an analogy to the Ising spin system in a neural network. We assumed that the modification of the synaptic strengths was dependent upon the correlation of pre- and post-synaptic neuronal firing. This assumption we denote as the Hebb hypothesis. In this paper, we solve exact\& a linearized version of the model and explicitly show that the capacity of the memory is related to the number of synapses rather than the much smaller number of neurons. In addition, we show that in order to utilize this large capacity, the network must store the major part of the information in the capability to generate patterns which evolve with time. We are also led to a modified Hebb hypothesis.},
	pages = {281--290},
	number = {3},
	journaltitle = {Mathematical Biosciences},
	shortjournal = {Mathematical Biosciences},
	author = {Little, W.A. and Shaw, Gordon L.},
	urldate = {2025-10-02},
	date = {1978-06},
	langid = {english},
	file = {PDF:/home/connorh/Zotero/storage/WP99PWFU/Little and Shaw - 1978 - Analytic study of the memory storage capacity of a neural network.pdf:application/pdf},
}

@article{newman_memory_1988,
	title = {Memory capacity in neural network models: Rigorous lower bounds},
	volume = {1},
	rights = {https://www.elsevier.com/tdm/userlicense/1.0/},
	issn = {08936080},
	url = {https://linkinghub.elsevier.com/retrieve/pii/0893608088900287},
	doi = {10.1016/0893-6080(88)90028-7},
	shorttitle = {Memory capacity in neural network models},
	abstract = {We consider certain simple neural network models of associative memory with N binary neurons and symmetric lth order synaptic connections, in which m randomly chosen N-bit patterns are to be stored and retrieved with a small fraction 6 of bit errors allowed. Rigorous proofs o f the following are presented both for l = 2 and l {\textgreater}- 3. 1. m can grow as fast as a N 1-1.},
	pages = {223--238},
	number = {3},
	journaltitle = {Neural Networks},
	shortjournal = {Neural Networks},
	author = {Newman, Charles M},
	urldate = {2025-10-02},
	date = {1988-01},
	langid = {english},
	file = {PDF:/home/connorh/Zotero/storage/KV36GF6K/Newman - 1988 - Memory capacity in neural network models Rigorous lower bounds.pdf:application/pdf},
}

@misc{kozachkov_neuron-astrocyte_2024,
	title = {Neuron-Astrocyte Associative Memory},
	url = {http://arxiv.org/abs/2311.08135},
	doi = {10.48550/arXiv.2311.08135},
	abstract = {Astrocytes, the most abundant type of glial cell, play a fundamental role in memory. Despite most hippocampal synapses being contacted by an astrocyte, there are no current theories that explain how neurons, synapses, and astrocytes might collectively contribute to memory function. We demonstrate that fundamental aspects of astrocyte morphology and physiology naturally lead to a dynamic, high-capacity associative memory system. The neuron-astrocyte networks generated by our framework are closely related to popular machine learning architectures known as Dense Associative Memories or Modern Hopfield Networks. In their known biological implementations the ratio of stored memories to the number of neurons remains constant, despite the growth of the network size. Our work demonstrates that neuron-astrocyte networks follow superior, supralinear memory scaling laws, outperforming all known biological implementations of Dense Associative Memory. This theoretical link suggests the exciting and previously unnoticed possibility that memories could be stored, at least in part, within astrocytes rather than solely in the synaptic weights between neurons.},
	number = {{arXiv}:2311.08135},
	publisher = {{arXiv}},
	author = {Kozachkov, Leo and Slotine, Jean-Jacques and Krotov, Dmitry},
	urldate = {2025-10-02},
	date = {2024-07-23},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2311.08135 [q-bio]},
	keywords = {Quantitative Biology - Neurons and Cognition},
	file = {PDF:/home/connorh/Zotero/storage/USAQ8L2G/Kozachkov et al. - 2024 - Neuron-Astrocyte Associative Memory.pdf:application/pdf},
}

@misc{hoover_dense_2024,
	title = {Dense Associative Memory Through the Lens of Random Features},
	rights = {Creative Commons Attribution Share Alike 4.0 International},
	url = {https://arxiv.org/abs/2410.24153},
	doi = {10.48550/ARXIV.2410.24153},
	abstract = {Dense Associative Memories are high storage capacity variants of the Hopfield networks that are capable of storing a large number of memory patterns in the weights of the network of a given size. Their common formulations typically require storing each pattern in a separate set of synaptic weights, which leads to the increase of the number of synaptic weights when new patterns are introduced. In this work we propose an alternative formulation of this class of models using random features, commonly used in kernel methods. In this formulation the number of network's parameters remains fixed. At the same time, new memories can be added to the network by modifying existing weights. We show that this novel network closely approximates the energy function and dynamics of conventional Dense Associative Memories and shares their desirable computational properties.},
	publisher = {{arXiv}},
	author = {Hoover, Benjamin and Chau, Duen Horng and Strobelt, Hendrik and Ram, Parikshit and Krotov, Dmitry},
	urldate = {2025-10-02},
	date = {2024},
	note = {Version Number: 1},
	keywords = {{FOS}: Computer and information sciences, Machine Learning (cs.{LG})},
	file = {PDF:/home/connorh/Zotero/storage/BAGYB89S/Hoover et al. - Dense Associative Memory Through the Lens of Random Features.pdf:application/pdf},
}

@article{oja_simplified_1982,
	title = {Simplified neuron model as a principal component analyzer},
	volume = {15},
	abstract = {A simple linear neuron model with constrained Hebbian-type synaptic modification is analyzed and a new class of unconstrained learning rules is derived. It is shown that the model neuron tends to extract the principal component from a stationary input vector sequence.},
	pages = {267--273},
	journaltitle = {Journal of Mathematical Biology},
	author = {Oja, Erkki},
	date = {1982},
	langid = {english},
	file = {PDF:/home/connorh/Zotero/storage/Z4EBMCT3/Oja - Simplified neuron model as a principal component analyzer.pdf:application/pdf},
}

@article{hinton_deterministic_1989,
	title = {Deterministic Boltzmann Learning Performs Steepest Descent in Weight-Space},
	volume = {1},
	pages = {143--510},
	journaltitle = {Neural Computation},
	author = {Hinton, Geoffrey E},
	date = {1989},
	langid = {english},
	file = {PDF:/home/connorh/Zotero/storage/77XSDN4B/Hinton - Deterministic Boltzmann Learning Performs Steepest Descent in Weight-Space.pdf:application/pdf},
}

@article{mishra_astrocytes_2016,
	title = {Astrocytes mediate neurovascular signaling to capillary pericytes but not to arterioles},
	volume = {19},
	issn = {1097-6256, 1546-1726},
	url = {https://www.nature.com/articles/nn.4428},
	doi = {10.1038/nn.4428},
	pages = {1619--1627},
	number = {12},
	journaltitle = {Nature Neuroscience},
	shortjournal = {Nat Neurosci},
	author = {Mishra, Anusha and Reynolds, James P and Chen, Yang and Gourine, Alexander V and Rusakov, Dmitri A and Attwell, David},
	urldate = {2025-10-08},
	date = {2016-12},
	langid = {english},
	file = {PDF:/home/connorh/Zotero/storage/HQW9X4DG/Mishra et al. - 2016 - Astrocytes mediate neurovascular signaling to capillary pericytes but not to arterioles.pdf:application/pdf},
}

@article{sun_organizing_2023,
	title = {Organizing memories for generalization in complementary learning systems},
	volume = {26},
	issn = {1097-6256, 1546-1726},
	url = {https://www.nature.com/articles/s41593-023-01382-9},
	doi = {10.1038/s41593-023-01382-9},
	abstract = {Abstract
            Memorization and generalization are complementary cognitive processes that jointly promote adaptive behavior. For example, animals should memorize safe routes to specific water sources and generalize from these memories to discover environmental features that predict new ones. These functions depend on systems consolidation mechanisms that construct neocortical memory traces from hippocampal precursors, but why systems consolidation only applies to a subset of hippocampal memories is unclear. Here we introduce a new neural network formalization of systems consolidation that reveals an overlooked tension—unregulated neocortical memory transfer can cause overfitting and harm generalization in an unpredictable world. We resolve this tension by postulating that memories only consolidate when it aids generalization. This framework accounts for partial hippocampal–cortical memory transfer and provides a normative principle for reconceptualizing numerous observations in the field. Generalization-optimized systems consolidation thus provides new insight into how adaptive behavior benefits from complementary learning systems specialized for memorization and generalization.},
	pages = {1438--1448},
	number = {8},
	journaltitle = {Nature Neuroscience},
	shortjournal = {Nat Neurosci},
	author = {Sun, Weinan and Advani, Madhu and Spruston, Nelson and Saxe, Andrew and Fitzgerald, James E.},
	urldate = {2025-10-08},
	date = {2023-08},
	langid = {english},
	file = {PDF:/home/connorh/Zotero/storage/BUWCWUCQ/Sun et al. - 2023 - Organizing memories for generalization in complementary learning systems.pdf:application/pdf},
}

@article{markram_regulation_1997,
	title = {Regulation of Synaptic Efficacy by Coincidence of Postsynaptic {APs} and {EPSPs}},
	volume = {275},
	issn = {0036-8075, 1095-9203},
	url = {https://www.science.org/doi/10.1126/science.275.5297.213},
	doi = {10.1126/science.275.5297.213},
	abstract = {Activity-driven modifications in synaptic connections between neurons in the neocortex may occur during development and learning. In dual whole-cell voltage recordings from pyramidal neurons, the coincidence of postsynaptic action potentials ({APs}) and unitary excitatory postsynaptic potentials ({EPSPs}) was found to induce changes in {EPSPs}. Their average amplitudes were differentially up- or down-regulated, depending on the precise timing of postsynaptic {APs} relative to {EPSPs}. These observations suggest that {APs} propagating back into dendrites serve to modify single active synaptic connections, depending on the pattern of electrical activity in the pre- and postsynaptic neurons.},
	pages = {213--215},
	number = {5297},
	journaltitle = {Science},
	shortjournal = {Science},
	author = {Markram, Henry and Lübke, Joachim and Frotscher, Michael and Sakmann, Bert},
	urldate = {2025-10-08},
	date = {1997-01-10},
	langid = {english},
	file = {PDF:/home/connorh/Zotero/storage/ZCR4NDQR/Markram et al. - 1997 - Regulation of Synaptic Efficacy by Coincidence of Postsynaptic APs and EPSPs.pdf:application/pdf},
}

@article{rolls_mechanisms_2013,
	title = {The mechanisms for pattern completion and pattern separation in the hippocampus},
	volume = {7},
	issn = {1662-5137},
	url = {http://journal.frontiersin.org/article/10.3389/fnsys.2013.00074/abstract},
	doi = {10.3389/fnsys.2013.00074},
	abstract = {The mechanisms for pattern completion and pattern separation are described in the context of a theory of hippocampal function in which the hippocampal {CA}3 system operates as a single attractor or autoassociation network to enable rapid, one-trial, associations between any spatial location (place in rodents, or spatial view in primates) and an object or reward, and to provide for completion of the whole memory during recall from any part. The factors important in the pattern completion in {CA}3 together with a large number of independent memories stored in {CA}3 include a sparse distributed representation which is enhanced by the graded ﬁring rates of {CA}3 neurons, representations that are independent due to the randomizing effect of the mossy ﬁbers, heterosynaptic long-term depression as well as long-term potentiation in the recurrent collateral synapses, and diluted connectivity to minimize the number of multiple synapses between any pair of {CA}3 neurons which otherwise distort the basins of attraction. Recall of information from {CA}3 is implemented by the entorhinal cortex perforant path synapses to {CA}3 cells, which in acting as a pattern associator allow some pattern generalization. Pattern separation is performed in the dentate granule cells using competitive learning to convert grid-like entorhinal cortex ﬁring to place-like ﬁelds. Pattern separation in {CA}3, which is important for completion of any one of the stored patterns from a fragment, is provided for by the randomizing effect of the mossy ﬁber synapses to which neurogenesis may contribute, by the large number of dentate granule cells each with a sparse representation, and by the sparse independent representations in {CA}3. Recall to the neocortex is achieved by a reverse hierarchical series of pattern association networks implemented by the hippocampo-cortical backprojections, each one of which performs some pattern generalization, to retrieve a complete pattern of cortical ﬁring in higher-order cortical areas.},
	journaltitle = {Frontiers in Systems Neuroscience},
	shortjournal = {Front. Syst. Neurosci.},
	author = {Rolls, Edmund T.},
	urldate = {2025-10-08},
	date = {2013},
	langid = {english},
	file = {PDF:/home/connorh/Zotero/storage/5W49N6H5/Rolls - 2013 - The mechanisms for pattern completion and pattern separation in the hippocampus.pdf:application/pdf},
}

@article{bittner_behavioral_2017,
	title = {Behavioral time scale synaptic plasticity underlies {CA}1 place fields},
	volume = {357},
	issn = {0036-8075, 1095-9203},
	url = {https://www.science.org/doi/10.1126/science.aan3846},
	doi = {10.1126/science.aan3846},
	abstract = {A different form of synaptic plasticity
            
              How do synaptic or other neuronal changes support learning? This subject has been dominated by Hebb's postulate of synaptic change. Although there is strong experimental support for Hebbian plasticity in a number of preparations, alternative ideas have also been developed over the years. Bittner
              et al.
              provide in vivo, in vitro, and modeling data to support the view that non-Hebbian plasticity may underlie the formation of hippocampal place fields (see the Perspective by Krupic). Instead of multiple pairings, a single strong Ca
              2+
              plateau potential in neuronal dendrites paired with spatial inputs may be sufficient to produce place cells.
            
            
              Science
              , this issue p.
              1033
              ; see also p.
              974
            
          , 
            A particular type of long–time scale plasticity shapes the formation of stable place fields in the hippocampus.
          , 
            
              Learning is primarily mediated by activity-dependent modifications of synaptic strength within neuronal circuits. We discovered that place fields in hippocampal area {CA}1 are produced by a synaptic potentiation notably different from Hebbian plasticity. Place fields could be produced in vivo in a single trial by potentiation of input that arrived seconds before and after complex spiking. The potentiated synaptic input was not initially coincident with action potentials or depolarization. This rule, named behavioral time scale synaptic plasticity, abruptly modifies inputs that were neither causal nor close in time to postsynaptic activation. In slices, five pairings of subthreshold presynaptic activity and calcium (Ca
              2+
              ) plateau potentials produced a large potentiation with an asymmetric seconds-long time course. This plasticity efficiently stores entire behavioral sequences within synaptic weights to produce predictive place cell activity.},
	pages = {1033--1036},
	number = {6355},
	journaltitle = {Science},
	shortjournal = {Science},
	author = {Bittner, Katie C. and Milstein, Aaron D. and Grienberger, Christine and Romani, Sandro and Magee, Jeffrey C.},
	urldate = {2025-10-09},
	date = {2017-09-08},
	langid = {english},
	file = {PDF:/home/connorh/Zotero/storage/XSR5QAJC/Bittner et al. - 2017 - Behavioral time scale synaptic plasticity underlies CA1 place fields.pdf:application/pdf},
}

@article{milstein_bidirectional_2021,
	title = {Bidirectional synaptic plasticity rapidly modifies hippocampal representations},
	volume = {10},
	issn = {2050-084X},
	url = {https://elifesciences.org/articles/73046},
	doi = {10.7554/eLife.73046},
	abstract = {Learning requires neural adaptations thought to be mediated by activity-dependent synaptic plasticity. A relatively non-standard form of synaptic plasticity driven by dendritic calcium spikes, or plateau potentials, has been reported to underlie place field formation in rodent hippocampal {CA}1 neurons. Here, we found that this behavioral timescale synaptic plasticity ({BTSP}) can also reshape existing place fields via bidirectional synaptic weight changes that depend on the temporal proximity of plateau potentials to pre-existing place fields. When evoked near an existing place field, plateau potentials induced less synaptic potentiation and more depression, suggesting {BTSP} might depend inversely on postsynaptic activation. However, manipulations of place cell membrane potential and computational modeling indicated that this anti-correlation actually results from a dependence on current synaptic weight such that weak inputs potentiate and strong inputs depress. A network model implementing this bidirectional synaptic learning rule suggested that {BTSP} enables population activity, rather than pairwise neuronal correlations, to drive neural adaptations to experience.},
	pages = {e73046},
	journaltitle = {{eLife}},
	author = {Milstein, Aaron D and Li, Yiding and Bittner, Katie C and Grienberger, Christine and Soltesz, Ivan and Magee, Jeffrey C and Romani, Sandro},
	urldate = {2025-10-09},
	date = {2021-12-09},
	langid = {english},
	file = {PDF:/home/connorh/Zotero/storage/C73JHILM/Milstein et al. - 2021 - Bidirectional synaptic plasticity rapidly modifies hippocampal representations.pdf:application/pdf},
}

@article{cheng_neural_1994,
	title = {Neural Networks: A Review from a Statistical Perspective},
	volume = {9},
	issn = {0883-4237},
	url = {https://projecteuclid.org/journals/statistical-science/volume-9/issue-1/Neural-Networks-A-Review-from-a-Statistical-Perspective/10.1214/ss/1177010638.full},
	doi = {10.1214/ss/1177010638},
	shorttitle = {Neural Networks},
	abstract = {This paper informs a statistical readership about Artificial Neural Networks ({ANNs}), points out some of the links with statistical methodology and encourages cross-disciplinary research in the directions most likely to bear fruit. The areas of statistical interest are briefly outlined, and a series of examples indicates the flavor of {ANN} models. We then treat various topics in more depth. In each case, we describe the neural network architectures and training rules and provide a statistical commentary. The topics treated in this way are perceptrons (from singleunit to multilayer versions), Hopfield-type recurrent networks (including probabilistic versions strongly related to statistical physics and Gibbs distributions) and associative memory networks trained by so-called unsuperviszd learning rules. Perceptrons are shown to have strong associations with discriminant analysis and regression, and unsupervized networks with cluster analysis. The paper concludes with some thoughts on the future of the interface between neural networks and statistics.},
	number = {1},
	journaltitle = {Statistical Science},
	shortjournal = {Statist. Sci.},
	author = {Cheng, Bing and Titterington, D. M.},
	urldate = {2025-10-09},
	date = {1994-02-01},
	langid = {english},
	file = {PDF:/home/connorh/Zotero/storage/BF55WQWH/Cheng and Titterington - 1994 - Neural Networks A Review from a Statistical Perspective.pdf:application/pdf},
}

@article{bao_capacity_2022,
	title = {The capacity of the dense associative memory networks},
	volume = {469},
	issn = {09252312},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0925231221015447},
	doi = {10.1016/j.neucom.2021.10.058},
	abstract = {This paper revisits the dense associative memory ({DAM}) networks and studies rigorously the capacity of the {DAM} networks. We present the capacity theorem of the {DAM} networks with an attraction radius or a noise level from the messages and prove that the probe can converge to the targeted message just after the one-step update. Under this convergence, the capacity of {DAM} networks is between a lower bound and an upper bound. Although when the attraction radius is 0:0 away from the messages, i.e. noiseless, previous literature provides an approximate result. However, a rigorous proof is not given in this study. In addition, we consider a more general notion of capacity which allows the retrieval of messages from noisy probes (the attraction radius is not 0:0). We demonstrates that the convergence result can be acquired just after the one-step update when the probe is a corrupted version with a Gaussian noise from one message. We further provide simulated experiments to validate theorems herein.},
	pages = {198--208},
	journaltitle = {Neurocomputing},
	shortjournal = {Neurocomputing},
	author = {Bao, Han and Zhang, Richong and Mao, Yongyi},
	urldate = {2025-10-09},
	date = {2022-01},
	langid = {english},
	file = {PDF:/home/connorh/Zotero/storage/P2NSIYPP/Bao et al. - 2022 - The capacity of the dense associative memory networks.pdf:application/pdf},
}

@article{mishra_astrocytes_2016-1,
	title = {Astrocytes mediate neurovascular signaling to capillary pericytes but not to arterioles},
	volume = {19},
	issn = {1097-6256, 1546-1726},
	url = {https://www.nature.com/articles/nn.4428},
	doi = {10.1038/nn.4428},
	pages = {1619--1627},
	number = {12},
	journaltitle = {Nature Neuroscience},
	shortjournal = {Nat Neurosci},
	author = {Mishra, Anusha and Reynolds, James P and Chen, Yang and Gourine, Alexander V and Rusakov, Dmitri A and Attwell, David},
	urldate = {2025-10-09},
	date = {2016-12},
	langid = {english},
	file = {PDF:/home/connorh/Zotero/storage/HWLI7K3S/Mishra et al. - 2016 - Astrocytes mediate neurovascular signaling to capillary pericytes but not to arterioles.pdf:application/pdf},
}

@book{hebb_organization_1949,
	location = {New York},
	edition = {1. issued in paperback},
	title = {The organization of behavior: a neuropsychological theory},
	isbn = {978-0-8058-4300-2 978-0-415-65453-1},
	shorttitle = {The organization of behavior},
	pagetotal = {21},
	publisher = {Routledge},
	author = {Hebb, Donald O.},
	date = {1949},
	langid = {english},
	file = {PDF:/home/connorh/Zotero/storage/CGIZI3HA/Hebb - 2012 - The organization of behavior a neuropsychological theory.pdf:application/pdf},
}

@article{lim_hebbian_2021,
	title = {Hebbian learning revisited and its inference underlying cognitive function},
	volume = {38},
	issn = {23521546},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S2352154621000280},
	doi = {10.1016/j.cobeha.2021.02.006},
	pages = {96--102},
	journaltitle = {Current Opinion in Behavioral Sciences},
	shortjournal = {Current Opinion in Behavioral Sciences},
	author = {Lim, Sukbin},
	urldate = {2025-10-09},
	date = {2021-04},
	langid = {english},
	file = {PDF:/home/connorh/Zotero/storage/IPLJKEG2/Lim - 2021 - Hebbian learning revisited and its inference underlying cognitive function.pdf:application/pdf},
}

@article{sun_organizing_2023-1,
	title = {Organizing memories for generalization in complementary learning systems},
	volume = {26},
	issn = {1097-6256, 1546-1726},
	url = {https://www.nature.com/articles/s41593-023-01382-9},
	doi = {10.1038/s41593-023-01382-9},
	abstract = {Abstract
            Memorization and generalization are complementary cognitive processes that jointly promote adaptive behavior. For example, animals should memorize safe routes to specific water sources and generalize from these memories to discover environmental features that predict new ones. These functions depend on systems consolidation mechanisms that construct neocortical memory traces from hippocampal precursors, but why systems consolidation only applies to a subset of hippocampal memories is unclear. Here we introduce a new neural network formalization of systems consolidation that reveals an overlooked tension—unregulated neocortical memory transfer can cause overfitting and harm generalization in an unpredictable world. We resolve this tension by postulating that memories only consolidate when it aids generalization. This framework accounts for partial hippocampal–cortical memory transfer and provides a normative principle for reconceptualizing numerous observations in the field. Generalization-optimized systems consolidation thus provides new insight into how adaptive behavior benefits from complementary learning systems specialized for memorization and generalization.},
	pages = {1438--1448},
	number = {8},
	journaltitle = {Nature Neuroscience},
	shortjournal = {Nat Neurosci},
	author = {Sun, Weinan and Advani, Madhu and Spruston, Nelson and Saxe, Andrew and Fitzgerald, James E.},
	urldate = {2025-10-09},
	date = {2023-08},
	langid = {english},
	file = {PDF:/home/connorh/Zotero/storage/I6DN33QU/Sun et al. - 2023 - Organizing memories for generalization in complementary learning systems.pdf:application/pdf},
}

@article{markram_regulation_1997-1,
	title = {Regulation of Synaptic Efficacy by Coincidence of Postsynaptic {APs} and {EPSPs}},
	volume = {275},
	issn = {0036-8075, 1095-9203},
	url = {https://www.science.org/doi/10.1126/science.275.5297.213},
	doi = {10.1126/science.275.5297.213},
	abstract = {Activity-driven modifications in synaptic connections between neurons in the neocortex may occur during development and learning. In dual whole-cell voltage recordings from pyramidal neurons, the coincidence of postsynaptic action potentials ({APs}) and unitary excitatory postsynaptic potentials ({EPSPs}) was found to induce changes in {EPSPs}. Their average amplitudes were differentially up- or down-regulated, depending on the precise timing of postsynaptic {APs} relative to {EPSPs}. These observations suggest that {APs} propagating back into dendrites serve to modify single active synaptic connections, depending on the pattern of electrical activity in the pre- and postsynaptic neurons.},
	pages = {213--215},
	number = {5297},
	journaltitle = {Science},
	shortjournal = {Science},
	author = {Markram, Henry and Lübke, Joachim and Frotscher, Michael and Sakmann, Bert},
	urldate = {2025-10-09},
	date = {1997-01-10},
	langid = {english},
	file = {PDF:/home/connorh/Zotero/storage/UUW43CZ3/Markram et al. - 1997 - Regulation of Synaptic Efficacy by Coincidence of Postsynaptic APs and EPSPs.pdf:application/pdf},
}

@article{bi_synaptic_1998,
	title = {Synaptic Modifications in Cultured Hippocampal Neurons: Dependence on Spike Timing, Synaptic Strength, and Postsynaptic Cell Type},
	volume = {18},
	rights = {https://creativecommons.org/licenses/by-nc-sa/4.0/},
	issn = {0270-6474, 1529-2401},
	url = {https://www.jneurosci.org/lookup/doi/10.1523/JNEUROSCI.18-24-10464.1998},
	doi = {10.1523/JNEUROSCI.18-24-10464.1998},
	shorttitle = {Synaptic Modifications in Cultured Hippocampal Neurons},
	abstract = {In cultures of dissociated rat hippocampal neurons, persistent potentiation and depression of glutamatergic synapses were induced by correlated spiking of presynaptic and postsynaptic neurons. The relative timing between the presynaptic and postsynaptic spiking determined the direction and the extent of synaptic changes. Repetitive postsynaptic spiking within a time window of 20 msec after presynaptic activation resulted in long-term potentiation ({LTP}), whereas postsynaptic spiking within a window of 20 msec before the repetitive presynaptic activation led to long-term depression ({LTD}). Significant {LTP} occurred only at synapses with relatively low initial strength, whereas the extent of {LTD} did not show obvious dependence on the initial synaptic strength. Both {LTP} and {LTD} depended on the activation of {NMDA} receptors and were absent in cases in which the postsynaptic neurons were {GABAergic} in nature. Blockade of L-type calcium channels with nimodipine abolished the induction of {LTD} and reduced the extent of {LTP}. These results underscore the importance of precise spike timing, synaptic strength, and postsynaptic cell type in the activity-induced modification of central synapses and suggest that Hebb’s rule may need to incorporate a quantitative consideration of spike timing that reflects the narrow and asymmetric window for the induction of synaptic modification.},
	pages = {10464--10472},
	number = {24},
	journaltitle = {The Journal of Neuroscience},
	shortjournal = {J. Neurosci.},
	author = {Bi, Guo-qiang and Poo, Mu-ming},
	urldate = {2025-10-09},
	date = {1998-12-15},
	langid = {english},
	file = {PDF:/home/connorh/Zotero/storage/X5P5SEC3/Bi and Poo - 1998 - Synaptic Modifications in Cultured Hippocampal Neurons Dependence on Spike Timing, Synaptic Strengt.pdf:application/pdf},
}

@article{rolls_mechanisms_2013-1,
	title = {The mechanisms for pattern completion and pattern separation in the hippocampus},
	volume = {7},
	issn = {1662-5137},
	url = {http://journal.frontiersin.org/article/10.3389/fnsys.2013.00074/abstract},
	doi = {10.3389/fnsys.2013.00074},
	abstract = {The mechanisms for pattern completion and pattern separation are described in the context of a theory of hippocampal function in which the hippocampal {CA}3 system operates as a single attractor or autoassociation network to enable rapid, one-trial, associations between any spatial location (place in rodents, or spatial view in primates) and an object or reward, and to provide for completion of the whole memory during recall from any part. The factors important in the pattern completion in {CA}3 together with a large number of independent memories stored in {CA}3 include a sparse distributed representation which is enhanced by the graded ﬁring rates of {CA}3 neurons, representations that are independent due to the randomizing effect of the mossy ﬁbers, heterosynaptic long-term depression as well as long-term potentiation in the recurrent collateral synapses, and diluted connectivity to minimize the number of multiple synapses between any pair of {CA}3 neurons which otherwise distort the basins of attraction. Recall of information from {CA}3 is implemented by the entorhinal cortex perforant path synapses to {CA}3 cells, which in acting as a pattern associator allow some pattern generalization. Pattern separation is performed in the dentate granule cells using competitive learning to convert grid-like entorhinal cortex ﬁring to place-like ﬁelds. Pattern separation in {CA}3, which is important for completion of any one of the stored patterns from a fragment, is provided for by the randomizing effect of the mossy ﬁber synapses to which neurogenesis may contribute, by the large number of dentate granule cells each with a sparse representation, and by the sparse independent representations in {CA}3. Recall to the neocortex is achieved by a reverse hierarchical series of pattern association networks implemented by the hippocampo-cortical backprojections, each one of which performs some pattern generalization, to retrieve a complete pattern of cortical ﬁring in higher-order cortical areas.},
	journaltitle = {Frontiers in Systems Neuroscience},
	shortjournal = {Front. Syst. Neurosci.},
	author = {Rolls, Edmund T.},
	urldate = {2025-10-09},
	date = {2013},
	langid = {english},
	file = {PDF:/home/connorh/Zotero/storage/KNJ2W4ZX/Rolls - 2013 - The mechanisms for pattern completion and pattern separation in the hippocampus.pdf:application/pdf},
}

@article{whittington_tolman-eichenbaum_2020,
	title = {The Tolman-Eichenbaum Machine: Unifying Space and Relational Memory through Generalization in the Hippocampal Formation},
	volume = {183},
	issn = {00928674},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S009286742031388X},
	doi = {10.1016/j.cell.2020.10.024},
	shorttitle = {The Tolman-Eichenbaum Machine},
	abstract = {The hippocampal-entorhinal system is important for spatial and relational memory tasks. We formally link these domains, provide a mechanistic understanding of the hippocampal role in generalization, and offer unifying principles underlying many entorhinal and hippocampal cell types. We propose medial entorhinal cells form a basis describing structural knowledge, and hippocampal cells link this basis with sensory representations. Adopting these principles, we introduce the Tolman-Eichenbaum machine ({TEM}). After learning, {TEM} entorhinal cells display diverse properties resembling apparently bespoke spatial responses, such as grid, band, border, and object-vector cells. {TEM} hippocampal cells include place and landmark cells that remap between environments. Crucially, {TEM} also aligns with empirically recorded representations in complex nonspatial tasks. {TEM} also generates predictions that hippocampal remapping is not random as previously believed; rather, structural knowledge is preserved across environments. We conﬁrm this structural transfer over remapping in simultaneously recorded place and grid cells.},
	pages = {1249--1263.e23},
	number = {5},
	journaltitle = {Cell},
	shortjournal = {Cell},
	author = {Whittington, James C.R. and Muller, Timothy H. and Mark, Shirley and Chen, Guifen and Barry, Caswell and Burgess, Neil and Behrens, Timothy E.J.},
	urldate = {2025-10-09},
	date = {2020-11},
	langid = {english},
	file = {PDF:/home/connorh/Zotero/storage/G463BP8W/Whittington et al. - 2020 - The Tolman-Eichenbaum Machine Unifying Space and Relational Memory through Generalization in the Hi.pdf:application/pdf},
}

@article{smolensky_tensor_1990,
	title = {Tensor product variable binding and the representation of symbolic structures in connectionist systems},
	volume = {46},
	issn = {0004-3702},
	url = {https://www.sciencedirect.com/science/article/pii/000437029090007M},
	doi = {10.1016/0004-3702(90)90007-M},
	abstract = {A general method, the tensor product representation, is defined for the connectionist representation of value/variable bindings. The technique is a formalization of the idea that a set of value/variable pairs can be represented by accumulating activity in a collection of units each of which computes the product of a feature of a variable and a feature of its value. The method allows the fully distributed representation of bindings and symbolic structures. Fully and partially localized special cases of the tensor product representation reduce to existing cases of connectionist representations of structured data. The representation rests on a principled analysis of structure; it saturates gracefully as larger structures are represented; it permits recursive construction of complex representations from simpler ones; it respects the independence of the capacities to generate and maintain multiple bindings in parallel; it extends naturally to continuous structures and continuous representational patterns; it permits values to also serve as variables; and it enables analysis of the interference of symbolic structures stored in associative memories. It has also served as the basis for working connectionist models of high-level cognitive tasks.},
	pages = {159--216},
	number = {1},
	journaltitle = {Artificial Intelligence},
	shortjournal = {Artificial Intelligence},
	author = {Smolensky, Paul},
	urldate = {2025-10-10},
	date = {1990-11-01},
	file = {ScienceDirect Snapshot:/home/connorh/Zotero/storage/9KHJWPMT/000437029090007M.html:text/html},
}

@article{plate_holographic_1995,
	title = {Holographic reduced representations},
	volume = {6},
	issn = {1941-0093},
	url = {https://ieeexplore.ieee.org/document/377968},
	doi = {10.1109/72.377968},
	abstract = {Associative memories are conventionally used to represent data with very simple structure: sets of pairs of vectors. This paper describes a method for representing more complex compositional structure in distributed representations. The method uses circular convolution to associate items, which are represented by vectors. Arbitrary variable bindings, short sequences of various lengths, simple frame-like structures, and reduced representations can be represented in a fixed width vector. These representations are items in their own right and can be used in constructing compositional structures. The noisy reconstructions extracted from convolution memories can be cleaned up by using a separate associative memory that has good reconstructive properties.{\textless}{\textgreater}},
	pages = {623--641},
	number = {3},
	journaltitle = {{IEEE} Transactions on Neural Networks},
	author = {Plate, T.A.},
	urldate = {2025-10-10},
	date = {1995-05},
	keywords = {Artificial intelligence, Associative memory, Convolution, Degradation, Cancer, Concrete, Councils, Holography, Tree data structures},
	file = {Snapshot:/home/connorh/Zotero/storage/GE2J5W3D/377968.html:text/html},
}

@online{gayler_multiplicative_1998,
	title = {Multiplicative Binding, Representation Operators \& Analogy (Workshop Poster)},
	url = {https://web-archive.southampton.ac.uk/cogprints.org/502/},
	abstract = {Analogical inference depends on systematic substitution of the components of compositional structures. Simple systematic substitution has been achieved in a number of connectionist systems that support binding (the ability to create connectionist representations of the combination of component representations). These systems have used various implementations of two generic composition operators: bind() and bundle(). This paper introduces a novel implementation of the bind() operator that is simple, can be efficiently implemented, and highlights the relationship between retrieval queries and analogical mapping. A frame of role/filler bindings can easily be represented using bind() and bundle(). However, typical binding systems are unable to adequately represent multiple frames and arbitrary nested compositional structures. A novel family of representational operators (called braid()) is introduced to address these problems. Other binding systems make the strong assumption that the roles and fillers are disjoint in order to avoid ambiguities inherent in their representational idioms. The braid() operator can be used to avoid this assumption. The new representational idiom suggests how the cognitive processes of bottom-up and top-down object recognition might be implemented. These processes depend on analogical mapping to integrate disjoint representations and drive perceptual search.},
	type = {Preprint},
	author = {Gayler, Ross W.},
	urldate = {2025-10-10},
	date = {1998},
	file = {Snapshot:/home/connorh/Zotero/storage/HBN43MAU/502.html:text/html},
}

@article{kelly_encoding_2013,
	title = {Encoding structure in holographic reduced representations},
	volume = {67},
	issn = {1878-7290},
	doi = {10.1037/a0030301},
	abstract = {Vector Symbolic Architectures ({VSAs}) such as Holographic Reduced Representations ({HRRs}) are computational associative memories used by cognitive psychologists to model behavioural and neurological aspects of human memory. We present a novel analysis of the mathematics of {VSAs} and a novel technique for representing data in {HRRs}. Encoding and decoding in {VSAs} can be characterised by Latin squares. Successful encoding requires the structure of the data to be orthogonal to the structure of the Latin squares. However, {HRRs} can successfully encode vectors of locally structured data if vectors are shuffled. Shuffling results are illustrated using images but are applicable to any nonrandom data. The ability to use locally structured vectors provides a technique for detailed modelling of stimuli in {HRR} models. ({PsycInfo} Database Record (c) 2025 {APA}, all rights reserved)},
	pages = {79--93},
	number = {2},
	journaltitle = {Canadian Journal of Experimental Psychology / Revue canadienne de psychologie expérimentale},
	author = {Kelly, Mary Alexandria and Blostein, Dorothea and Mewhort, D. J. K.},
	date = {2013},
	note = {Place: {US}
Publisher: Educational Publishing Foundation},
	keywords = {Memory, Cognitive Psychology, Associative Memory, Associative Processes, Computational Modeling, Mathematics},
	file = {Snapshot:/home/connorh/Zotero/storage/B3GB9NX9/doiLanding.html:text/html;Submitted Version:/home/connorh/Zotero/storage/TIBZAUGI/Kelly et al. - 2013 - Encoding structure in holographic reduced representations.pdf:application/pdf},
}

@article{lansner_benchmarking_nodate,
	title = {Benchmarking Hebbian learning rules for associative memory},
	abstract = {Associative memory or content addressable memory is an important component function in computer science and information processing, and at the same time it is a key concept in cognitive and computational brain science. Many different neural network architectures and learning rules have been proposed to model the brain’s associative memory while investigating key component functions like pattern completion and rivalry together with noise reduction. A less investigated but equally important capability of active memory is prototype extraction where the training set comprises pattern instances generated by distorting prototype patterns and the task of the trained network is to recall the generating prototype given a new instance. In this paper we benchmark the associative memory function of seven different Hebbian learning rules employed in non-modular and modular recurrent networks with winner-take-all dynamic operating on moderately sparse binary patterns. Overall, we find that the modular networks have largest memory quantified as pattern storage capacity. The popular standard Hebb rule comes out with worst capacity while covariance learning proves to be robust but have low capacity, and the Bayesian-Hebbian rules show highest pattern storage capacity under the different conditions tested.},
	author = {Lansner, Anders and Ravichandran, Naresh B and Knoblauch, Andreas},
	langid = {english},
	file = {PDF:/home/connorh/Zotero/storage/WFR52RZ3/Lansner et al. - Benchmarking Hebbian learning rules for associative memory.pdf:application/pdf},
}

@misc{lansner_benchmarking_2025,
	title = {Benchmarking Hebbian learning rules for associative memory},
	url = {http://arxiv.org/abs/2401.00335},
	doi = {10.48550/arXiv.2401.00335},
	abstract = {Associative memory or content addressable memory is an important component function in computer science and information processing and is a key concept in cognitive and computational brain science. Many different neural network architectures and learning rules have been proposed to model associative memory of the brain while investigating key functions like pattern completion and rivalry, noise reduction, and storage capacity. A less investigated but important function is prototype extraction where the training set comprises pattern instances generated by distorting prototype patterns and the task of the trained network is to recall the correct prototype pattern given a new instance. In this paper we characterize these different aspects of associative memory performance and benchmark six different learning rules on storage capacity and prototype extraction. We consider only models with Hebbian plasticity that operate on sparse distributed representations with unit activities in the interval [0,1]. We evaluate both non-modular and modular network architectures and compare performance when trained and tested on different kinds of sparse random binary pattern sets, including correlated ones. We show that covariance learning has a robust but low storage capacity under these conditions and that the Bayesian Confidence Propagation learning rule ({BCPNN}) is superior with a good margin in all cases except one, reaching a three times higher composite score than the second best learning rule tested.},
	number = {{arXiv}:2401.00335},
	publisher = {{arXiv}},
	author = {Lansner, Anders and Ravichandran, Naresh B. and Herman, Pawel},
	urldate = {2025-10-11},
	date = {2025-02-17},
	eprinttype = {arxiv},
	eprint = {2401.00335 [cs]},
	keywords = {Computer Science - Neural and Evolutionary Computing},
	file = {Preprint PDF:/home/connorh/Zotero/storage/544K6P2B/Lansner et al. - 2025 - Benchmarking Hebbian learning rules for associative memory.pdf:application/pdf;Snapshot:/home/connorh/Zotero/storage/D4MGQNG8/2401.html:text/html},
}

@misc{hoover_energy_2023,
	title = {Energy Transformer},
	url = {http://arxiv.org/abs/2302.07253},
	doi = {10.48550/arXiv.2302.07253},
	abstract = {Our work combines aspects of three promising paradigms in machine learning, namely, attention mechanism, energy-based models, and associative memory. Attention is the power-house driving modern deep learning successes, but it lacks clear theoretical foundations. Energy-based models allow a principled approach to discriminative and generative tasks, but the design of the energy functional is not straightforward. At the same time, Dense Associative Memory models or Modern Hopfield Networks have a well-established theoretical foundation, and allow an intuitive design of the energy function. We propose a novel architecture, called the Energy Transformer (or {ET} for short), that uses a sequence of attention layers that are purposely designed to minimize a specifically engineered energy function, which is responsible for representing the relationships between the tokens. In this work, we introduce the theoretical foundations of {ET}, explore its empirical capabilities using the image completion task, and obtain strong quantitative results on the graph anomaly detection and graph classification tasks.},
	number = {{arXiv}:2302.07253},
	publisher = {{arXiv}},
	author = {Hoover, Benjamin and Liang, Yuchen and Pham, Bao and Panda, Rameswar and Strobelt, Hendrik and Chau, Duen Horng and Zaki, Mohammed J. and Krotov, Dmitry},
	urldate = {2025-10-11},
	date = {2023-11-01},
	eprinttype = {arxiv},
	eprint = {2302.07253 [cs]},
	keywords = {Quantitative Biology - Neurons and Cognition, Computer Science - Machine Learning, Statistics - Machine Learning, Condensed Matter - Disordered Systems and Neural Networks, Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:/home/connorh/Zotero/storage/8CRY5AZR/Hoover et al. - 2023 - Energy Transformer.pdf:application/pdf;Snapshot:/home/connorh/Zotero/storage/T8JA2SWU/2302.html:text/html},
}

@misc{hoover_memory_2024,
	title = {Memory in Plain Sight: Surveying the Uncanny Resemblances of Associative Memories and Diffusion Models},
	url = {http://arxiv.org/abs/2309.16750},
	doi = {10.48550/arXiv.2309.16750},
	shorttitle = {Memory in Plain Sight},
	abstract = {The generative process of Diffusion Models ({DMs}) has recently set state-of-the-art on many {AI} generation benchmarks. Though the generative process is traditionally understood as an "iterative denoiser", there is no universally accepted language to describe it. We introduce a novel perspective to describe {DMs} using the mathematical language of memory retrieval from the field of energy-based Associative Memories ({AMs}), making efforts to keep our presentation approachable to newcomers to both of these fields. Unifying these two fields provides insight that {DMs} can be seen as a particular kind of {AM} where Lyapunov stability guarantees are bypassed by intelligently engineering the dynamics (i.e., the noise and step size schedules) of the denoising process. Finally, we present a growing body of evidence that records {DMs} exhibiting empirical behavior we would expect from {AMs}, and conclude by discussing research opportunities that are revealed by understanding {DMs} as a form of energy-based memory.},
	number = {{arXiv}:2309.16750},
	publisher = {{arXiv}},
	author = {Hoover, Benjamin and Strobelt, Hendrik and Krotov, Dmitry and Hoffman, Judy and Kira, Zsolt and Chau, Duen Horng},
	urldate = {2025-10-11},
	date = {2024-05-28},
	eprinttype = {arxiv},
	eprint = {2309.16750 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Mathematics - Dynamical Systems},
	file = {Preprint PDF:/home/connorh/Zotero/storage/V2RZGNT4/Hoover et al. - 2024 - Memory in Plain Sight Surveying the Uncanny Resemblances of Associative Memories and Diffusion Mode.pdf:application/pdf;Snapshot:/home/connorh/Zotero/storage/VW3S9TQB/2309.html:text/html},
}

@incollection{sejnowski_hebb_1989,
	title = {The Hebb Rule for Synaptic Plasticity: Algorithms and},
	booktitle = {Neural Models of Plasticity},
	publisher = {Academic Press},
	author = {Sejnowski, Terrence J and Tesauro, Gerald},
	date = {1989},
	langid = {english},
	file = {PDF:/home/connorh/Zotero/storage/ZH94E3FV/Sejnowski and Tesauro - The Hebb Rule for Synaptic Plasticity Algorithms and.pdf:application/pdf},
}

@article{qiu_unbiased_2020,
	title = {{UNBIASED} {CONTRASTIVE} {DIVERGENCE} {ALGORITHM} {FOR} {TRAINING} {ENERGY}-{BASED} {LATENT} {VARIABLE} {MODELS}},
	abstract = {The contrastive divergence algorithm is a popular approach to training energybased latent variable models, which has been widely used in many machine learning models such as the restricted Boltzmann machines and deep belief nets. Despite its empirical success, the contrastive divergence algorithm is also known to have biases that severely affect its convergence. In this article we propose an unbiased version of the contrastive divergence algorithm that completely removes its bias in stochastic gradient methods, based on recent advances on unbiased Markov chain Monte Carlo methods. Rigorous theoretical analysis is developed to justify the proposed algorithm, and numerical experiments show that it signiﬁcantly improves the existing method. Our ﬁndings suggest that the unbiased contrastive divergence algorithm is a promising approach to training general energy-based latent variable models.},
	author = {Qiu, Yixuan and Zhang, Lingsong and Wang, Xiao},
	date = {2020},
	langid = {english},
	file = {PDF:/home/connorh/Zotero/storage/LA5I2X4B/Qiu et al. - 2020 - UNBIASED CONTRASTIVE DIVERGENCE ALGORITHM FOR TRAINING ENERGY-BASED LATENT VARIABLE MODELS.pdf:application/pdf},
}

@article{rumelhart_learning_1986,
	title = {Learning representations by back-propagating errors},
	volume = {323},
	rights = {http://www.springer.com/tdm},
	issn = {0028-0836, 1476-4687},
	url = {https://www.nature.com/articles/323533a0},
	doi = {10.1038/323533a0},
	pages = {533--536},
	number = {6088},
	journaltitle = {Nature},
	shortjournal = {Nature},
	author = {Rumelhart, David E. and Hinton, Geoffrey E. and Williams, Ronald J.},
	urldate = {2025-10-14},
	date = {1986-10},
	langid = {english},
}

@article{spratling_review_2017,
	title = {A review of predictive coding algorithms},
	volume = {112},
	issn = {0278-2626},
	url = {https://www.sciencedirect.com/science/article/pii/S027826261530035X},
	doi = {10.1016/j.bandc.2015.11.003},
	series = {Perspectives on Human Probabilistic Inferences and the 'Bayesian Brain'},
	abstract = {Predictive coding is a leading theory of how the brain performs probabilistic inference. However, there are a number of distinct algorithms which are described by the term “predictive coding”. This article provides a concise review of these different predictive coding algorithms, highlighting their similarities and differences. Five algorithms are covered: linear predictive coding which has a long and influential history in the signal processing literature; the first neuroscience-related application of predictive coding to explaining the function of the retina; and three versions of predictive coding that have been proposed to model cortical function. While all these algorithms aim to fit a generative model to sensory data, they differ in the type of generative model they employ, in the process used to optimise the fit between the model and sensory data, and in the way that they are related to neurobiology.},
	pages = {92--97},
	journaltitle = {Brain and Cognition},
	shortjournal = {Brain and Cognition},
	author = {Spratling, M. W.},
	urldate = {2025-10-14},
	date = {2017-03-01},
	keywords = {Cortex, Free energy, Neural networks, Predictive coding, Retina, Signal processing},
	file = {ScienceDirect Snapshot:/home/connorh/Zotero/storage/3FELPFQS/S027826261530035X.html:text/html;Submitted Version:/home/connorh/Zotero/storage/AMDIX85N/Spratling - 2017 - A review of predictive coding algorithms.pdf:application/pdf},
}

@incollection{munakata_how_2006,
	title = {How far can you go with Hebbian learning, and when does it lead you astray?},
	isbn = {978-0-19-856874-2 978-1-383-02987-1},
	url = {https://academic.oup.com/book/54488/chapter/422571559},
	abstract = {This paper considers the use of Hebbian learning rules to model aspects of development and learning, including the emergence of structure in the visual system in early life. There is considerable physiological evidence that a Hebb-like learning rule applies to the strengthening of synaptic efficacy seen in neurophysiological investigations of synaptic plasticity, and similar learning rules are often used to show how various properties of visual neurons and their organization into ocular dominance stripes and orientation columns could arise without being otherwise preprogrammed. Some of the plusses and minuses of Hebbian learning are considered. Hebbian learning can strengthen the neural response that is elicited by an input; this can be useful if the response made is appropriate to the situation, but it can also be counterproductive if a different response would be more appropriate. Examples in which this outcome-independent Hebbian type of strengthening might account, at least in part, for cases in which humans fail to learn are considered, and computational models embodying the Hebbian approach are described that can account for the findings. At a systems level, Hebbian learning cannot be the whole story. From a computational point of view, Hebbian learning can certainly lead one in the wrong direction, and some form of control over this is necessary. Also, experimental findings clearly show that human learning can be affected by accuracy or outcome feedback. Several ways in which sensitivity to feedback might be incorporated to guide learning within a fundamentally Hebbian framework for learning are considered.},
	pages = {33--60},
	booktitle = {Processes of Change in Brain and Cognitive Development},
	publisher = {Oxford University {PressOxford}},
	author = {{McClelland}, James L},
	editor = {Munakata, Yuko and Johnson, Mark H},
	urldate = {2025-10-14},
	date = {2006-04-06},
	langid = {english},
	doi = {10.1093/oso/9780198568742.003.0002},
	file = {PDF:/home/connorh/Zotero/storage/GA9UBUMU/McClelland - 2006 - How far can you go with Hebbian learning, and when does it lead you astray.pdf:application/pdf},
}
