{"rule":"MORFOLOGIK_RULE_EN_US","sentence":"^\\QA recent revival of interest AMs in machine learning research, driven by their equivalence with “attention” layers in the transformer architecture vaswani_attention_2023, ramsauer_hopfield_2021, has led to drastic advances in the storage capacity of AMs demircigil_model_2017,krotov_dense_2016,hu_provably_2024.\\E$"}
{"rule":"MORFOLOGIK_RULE_EN_US","sentence":"^\\QTraditional models of Associative Memory rely on Hebbian update rules to memorize stored patterns in the weights of lateral connections amari_learning_1972,hopfield_neural_1982,kohonen_correlation_1988, nakano_associatron-model_1972,.\\E$"}
